{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pix2Pix è®­ç»ƒå®éªŒ - Google Colab è®¾ç½®\n",
        "\n",
        "æœ¬ notebook ç”¨äºåœ¨ Google Colab ä¸Šè¿è¡Œ Pix2Pix (L1 + GAN) è®­ç»ƒå®éªŒï¼ˆE2ï¼‰ã€‚\n",
        "\n",
        "## ä½¿ç”¨è¯´æ˜\n",
        "\n",
        "1. **ä¸Šä¼ ä»£ç åˆ° GitHub/Gitee**ï¼ˆæ¨èï¼‰æˆ–ç›´æ¥ä¸Šä¼  zip åˆ° Colab\n",
        "2. **åœ¨ Google Drive ä¸Šå‡†å¤‡æ•°æ®**ï¼š\n",
        "   - å°† `processed` å’Œ `splits` æ–‡ä»¶å¤¹å‹ç¼©æˆ `data.zip`\n",
        "   - ä¸Šä¼  `data.zip` åˆ° Google Driveï¼ˆå»ºè®®æ”¾åœ¨ Drive æ ¹ç›®å½•ï¼‰\n",
        "3. **ä¿®æ”¹ä¸‹é¢çš„ `DRIVE_DATA_ZIP`**ï¼šæ”¹ä¸ºä½ çš„ `data.zip` åœ¨ Drive ä¸­çš„è·¯å¾„\n",
        "4. **ç‚¹å‡» \"Run all\"** è¿è¡Œæ‰€æœ‰ cell\n",
        "\n",
        "## æ•°æ®å‡†å¤‡è¯´æ˜\n",
        "\n",
        "- **å‹ç¼©æ–¹å¼**ï¼šå°† `data/processed` å’Œ `data/splits` ä¸¤ä¸ªæ–‡ä»¶å¤¹å‹ç¼©æˆ `data.zip`\n",
        "- **ä¸Šä¼ ä½ç½®**ï¼šå»ºè®®ä¸Šä¼ åˆ° Drive æ ¹ç›®å½• `/content/drive/MyDrive/data.zip`\n",
        "- **è§£å‹åç»“æ„**ï¼š`data/processed/train/{photo,label}`, `data/processed/val/{photo,label}`, `data/splits/cityscapes_split_seed42.json`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. å®‰è£…ä¾èµ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„ Python åŒ…\n",
        "!pip install -q tqdm pillow torchvision scikit-image torchmetrics\n",
        "\n",
        "# å¦‚æœéœ€è¦ Perceptual Lossï¼ˆå¯é€‰ï¼‰\n",
        "# !pip install -q lpips\n",
        "\n",
        "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. æŒ‚è½½ Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ… Google Drive æŒ‚è½½å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. å…‹éš†ä»£ç ä»“åº“ï¼ˆæˆ–ä¸Šä¼ ä»£ç ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ–¹å¼1ï¼šä» GitHub/Gitee å…‹éš†ï¼ˆæ¨èï¼‰\n",
        "# è¯·å°†ä¸‹é¢çš„ URL æ›¿æ¢ä¸ºä½ çš„ä»“åº“åœ°å€\n",
        "# !git clone https://github.com/zhuxie0719/Image-to-Image-Translation-Experiment\n",
        "# %cd Image-to-Image-Translation-Experiment\n",
        "\n",
        "# æ–¹å¼2ï¼šå¦‚æœä»£ç å·²ç»åœ¨ Colab ä¸­ï¼ˆé€šè¿‡ä¸Šä¼  zip ç­‰æ–¹å¼ï¼‰\n",
        "# ç›´æ¥ä½¿ç”¨å½“å‰ç›®å½•ï¼Œè·³è¿‡è¿™ä¸€æ­¥\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰\n",
        "PROJECT_ROOT = Path(\"/content/Image-to-Image-Translation-Experiment\")\n",
        "\n",
        "# å¦‚æœé¡¹ç›®ä¸åœ¨ /content ä¸‹ï¼Œè¯·ä¿®æ”¹ä¸Šé¢çš„è·¯å¾„\n",
        "# ä¾‹å¦‚ï¼šPROJECT_ROOT = Path(\"/content/drive/MyDrive/Image-to-Image-Translation-Experiment\")\n",
        "\n",
        "if PROJECT_ROOT.exists():\n",
        "    os.chdir(PROJECT_ROOT)\n",
        "    print(f\"âœ… é¡¹ç›®ç›®å½•ï¼š{PROJECT_ROOT}\")\n",
        "    \n",
        "    # å¦‚æœæ˜¯ä» Git å…‹éš†çš„ï¼Œå°è¯•æ›´æ–°ä»£ç ï¼ˆå…ˆæ¸…ç† __pycache__ï¼‰\n",
        "    if (PROJECT_ROOT / \".git\").exists():\n",
        "        print(\"æ£€æµ‹åˆ° Git ä»“åº“ï¼Œæ¸…ç† __pycache__ åæ›´æ–°ä»£ç ...\")\n",
        "        # åˆ é™¤æ‰€æœ‰ __pycache__ ç›®å½•\n",
        "        for pycache_dir in PROJECT_ROOT.rglob(\"__pycache__\"):\n",
        "            shutil.rmtree(pycache_dir)\n",
        "            print(f\"  åˆ é™¤ï¼š{pycache_dir}\")\n",
        "        \n",
        "        # ç°åœ¨å¯ä»¥å®‰å…¨åœ° pull\n",
        "        try:\n",
        "            result = subprocess.run([\"git\", \"pull\"], capture_output=True, text=True)\n",
        "            if result.returncode == 0:\n",
        "                print(\"âœ… ä»£ç æ›´æ–°æˆåŠŸ\")\n",
        "            else:\n",
        "                print(f\"âš ï¸  Git pull è¾“å‡ºï¼š{result.stdout}\")\n",
        "                print(f\"âš ï¸  Git pull é”™è¯¯ï¼š{result.stderr}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Git pull å¤±è´¥ï¼š{e}ï¼Œä½†å¯ä»¥ç»§ç»­ä½¿ç”¨å½“å‰ä»£ç \")\n",
        "else:\n",
        "    print(f\"âš ï¸  é¡¹ç›®ç›®å½•ä¸å­˜åœ¨ï¼š{PROJECT_ROOT}\")\n",
        "    print(\"è¯·å…ˆä¸Šä¼ ä»£ç æˆ–å…‹éš†ä»“åº“\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. è§£å‹æ•°æ®æ–‡ä»¶ï¼ˆdata.zipï¼‰\n",
        "\n",
        "å¦‚æœä½ å·²ç»å°† `data.zip` ä¸Šä¼ åˆ° Google Driveï¼Œè¯·å…ˆè§£å‹å®ƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âš ï¸ é‡è¦ï¼šè¯·ä¿®æ”¹ä¸‹é¢çš„è·¯å¾„ä¸ºä½ çš„ data.zip åœ¨ Google Drive ä¸­çš„ä½ç½®\n",
        "# ä¾‹å¦‚ï¼šå¦‚æœ data.zip åœ¨ Drive æ ¹ç›®å½•ï¼Œè·¯å¾„æ˜¯ï¼š/content/drive/MyDrive/data.zip\n",
        "DRIVE_DATA_ZIP = \"/content/drive/MyDrive/data.zip\"\n",
        "\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# æ£€æŸ¥ zip æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "if Path(DRIVE_DATA_ZIP).exists():\n",
        "    print(f\"âœ… æ‰¾åˆ° data.zipï¼š{DRIVE_DATA_ZIP}\")\n",
        "    \n",
        "    # è§£å‹åˆ°é¡¹ç›®ç›®å½•ä¸‹çš„ data æ–‡ä»¶å¤¹\n",
        "    project_data_dir = PROJECT_ROOT / \"data\"\n",
        "    project_data_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    print(f\"æ­£åœ¨è§£å‹åˆ°ï¼š{project_data_dir}\")\n",
        "    with zipfile.ZipFile(DRIVE_DATA_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(project_data_dir)\n",
        "    \n",
        "    print(\"âœ… è§£å‹å®Œæˆ\")\n",
        "    \n",
        "    # éªŒè¯è§£å‹åçš„ç»“æ„\n",
        "    required_dirs = [\n",
        "        \"processed/train/photo\",\n",
        "        \"processed/train/label\",\n",
        "        \"processed/val/photo\",\n",
        "        \"processed/val/label\",\n",
        "        \"splits\"\n",
        "    ]\n",
        "    \n",
        "    missing_dirs = []\n",
        "    for dir_name in required_dirs:\n",
        "        if not (project_data_dir / dir_name).exists():\n",
        "            missing_dirs.append(dir_name)\n",
        "    \n",
        "    if missing_dirs:\n",
        "        print(f\"âš ï¸  è§£å‹åç¼ºå°‘ä»¥ä¸‹ç›®å½•ï¼š{missing_dirs}\")\n",
        "        print(\"è¯·æ£€æŸ¥ zip æ–‡ä»¶ç»“æ„æ˜¯å¦æ­£ç¡®\")\n",
        "    else:\n",
        "        print(\"âœ… è§£å‹åçš„ç›®å½•ç»“æ„æ­£ç¡®\")\n",
        "        \n",
        "        # æ£€æŸ¥åˆ’åˆ†æ–‡ä»¶\n",
        "        split_file = project_data_dir / \"splits\" / \"cityscapes_split_seed42.json\"\n",
        "        if split_file.exists():\n",
        "            print(f\"âœ… åˆ’åˆ†æ–‡ä»¶å­˜åœ¨ï¼š{split_file}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨ï¼š{split_file}\")\n",
        "            \n",
        "        # ç»Ÿè®¡æ–‡ä»¶æ•°é‡\n",
        "        train_photos = list((project_data_dir / 'processed' / 'train' / 'photo').glob('*.jpg'))\n",
        "        val_photos = list((project_data_dir / 'processed' / 'val' / 'photo').glob('*.jpg'))\n",
        "        print(f\"   è®­ç»ƒé›† photo æ•°é‡ï¼š{len(train_photos)} å¼ \")\n",
        "        print(f\"   éªŒè¯é›† photo æ•°é‡ï¼š{len(val_photos)} å¼ \")\n",
        "else:\n",
        "    print(f\"âš ï¸  data.zip ä¸å­˜åœ¨ï¼š{DRIVE_DATA_ZIP}\")\n",
        "    print(\"è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å…ˆä¸Šä¼  data.zip åˆ° Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. è®¾ç½®æ•°æ®è·¯å¾„ï¼ˆå¦‚æœæ•°æ®å·²è§£å‹ï¼Œå¯è·³è¿‡æ­¤æ­¥éª¤ï¼‰\n",
        "\n",
        "å¦‚æœä½ å·²ç»è§£å‹äº† data.zipï¼ˆä¸Šé¢çš„æ­¥éª¤ï¼‰ï¼Œæ•°æ®å·²ç»åœ¨é¡¹ç›®ç›®å½•ä¸‹äº†ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸ªæ­¥éª¤ã€‚\n",
        "\n",
        "å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å…¶ä»–æ–¹å¼å‡†å¤‡çš„æ•°æ®ï¼Œè¯·åœ¨è¿™é‡Œè®¾ç½®è·¯å¾„ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. åˆ›å»ºæ•°æ®è½¯é“¾æ¥ï¼ˆå¦‚æœæ•°æ®ä¸åœ¨é¡¹ç›®ç›®å½•ä¸‹ï¼‰\n",
        "\n",
        "å¦‚æœæ•°æ®åœ¨ Google Drive çš„å…¶ä»–ä½ç½®ï¼ˆè€Œä¸æ˜¯é¡¹ç›®ç›®å½•ä¸‹ï¼‰ï¼Œéœ€è¦åˆ›å»ºè½¯é“¾æ¥ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âš ï¸ é‡è¦ï¼šè¯·ä¿®æ”¹ä¸‹é¢çš„è·¯å¾„ä¸ºä½ çš„ Google Drive æ•°æ®ç›®å½•\n",
        "# ä¾‹å¦‚ï¼š/content/drive/MyDrive/Cityscapes/Image-to-Image-Translation-Experiment/data\n",
        "DRIVE_DATA_PATH = \"/content/drive/MyDrive/Cityscapes/Image-to-Image-Translation-Experiment/data\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# æ£€æŸ¥ Drive æ•°æ®è·¯å¾„æ˜¯å¦å­˜åœ¨\n",
        "drive_data_path = Path(DRIVE_DATA_PATH)\n",
        "if not drive_data_path.exists():\n",
        "    print(f\"âš ï¸  Drive æ•°æ®è·¯å¾„ä¸å­˜åœ¨ï¼š{DRIVE_DATA_PATH}\")\n",
        "    print(\"è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å…ˆä¸Šä¼ æ•°æ®åˆ° Drive\")\n",
        "else:\n",
        "    print(f\"âœ… Drive æ•°æ®è·¯å¾„å­˜åœ¨ï¼š{DRIVE_DATA_PATH}\")\n",
        "    \n",
        "    # æ£€æŸ¥å¿…è¦çš„å­ç›®å½•\n",
        "    required_dirs = [\n",
        "        \"processed/train/photo\",\n",
        "        \"processed/train/label\",\n",
        "        \"processed/val/photo\",\n",
        "        \"processed/val/label\",\n",
        "        \"splits\"\n",
        "    ]\n",
        "    \n",
        "    missing_dirs = []\n",
        "    for dir_name in required_dirs:\n",
        "        if not (drive_data_path / dir_name).exists():\n",
        "            missing_dirs.append(dir_name)\n",
        "    \n",
        "    if missing_dirs:\n",
        "        print(f\"âš ï¸  ç¼ºå°‘ä»¥ä¸‹ç›®å½•ï¼š{missing_dirs}\")\n",
        "    else:\n",
        "        print(\"âœ… æ‰€æœ‰å¿…è¦çš„æ•°æ®ç›®å½•éƒ½å­˜åœ¨\")\n",
        "    \n",
        "    # æ£€æŸ¥åˆ’åˆ†æ–‡ä»¶\n",
        "    split_file = drive_data_path / \"splits\" / \"cityscapes_split_seed42.json\"\n",
        "    if split_file.exists():\n",
        "        print(f\"âœ… åˆ’åˆ†æ–‡ä»¶å­˜åœ¨ï¼š{split_file}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  åˆ’åˆ†æ–‡ä»¶ä¸å­˜åœ¨ï¼š{split_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºæŒ‡å‘ Drive æ•°æ®ç›®å½•çš„è½¯é“¾æ¥\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "project_data_dir = PROJECT_ROOT / \"data\"\n",
        "\n",
        "# å¦‚æœå·²å­˜åœ¨ data ç›®å½•ï¼ˆå¯èƒ½æ˜¯è½¯é“¾æ¥æˆ–çœŸå®ç›®å½•ï¼‰ï¼Œå…ˆåˆ é™¤\n",
        "if project_data_dir.exists() or project_data_dir.is_symlink():\n",
        "    if project_data_dir.is_symlink():\n",
        "        project_data_dir.unlink()\n",
        "        print(f\"åˆ é™¤æ—§çš„è½¯é“¾æ¥ï¼š{project_data_dir}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  {project_data_dir} å·²å­˜åœ¨ä¸”ä¸æ˜¯è½¯é“¾æ¥ï¼Œè¯·æ‰‹åŠ¨å¤„ç†\")\n",
        "\n",
        "# åˆ›å»ºè½¯é“¾æ¥\n",
        "if not project_data_dir.exists():\n",
        "    os.symlink(DRIVE_DATA_PATH, project_data_dir)\n",
        "    print(f\"âœ… åˆ›å»ºè½¯é“¾æ¥ï¼š{project_data_dir} -> {DRIVE_DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"âœ… æ•°æ®ç›®å½•å·²å­˜åœ¨ï¼š{project_data_dir}\")\n",
        "\n",
        "# éªŒè¯è½¯é“¾æ¥\n",
        "if (project_data_dir / \"processed\" / \"train\" / \"photo\").exists():\n",
        "    print(\"âœ… æ•°æ®è½¯é“¾æ¥éªŒè¯æˆåŠŸ\")\n",
        "    train_photos = list((project_data_dir / 'processed' / 'train' / 'photo').glob('*.jpg'))\n",
        "    val_photos = list((project_data_dir / 'processed' / 'val' / 'photo').glob('*.jpg'))\n",
        "    print(f\"   è®­ç»ƒé›† photo æ•°é‡ï¼š{len(train_photos)} å¼ \")\n",
        "    print(f\"   éªŒè¯é›† photo æ•°é‡ï¼š{len(val_photos)} å¼ \")\n",
        "else:\n",
        "    print(\"âš ï¸  æ•°æ®è½¯é“¾æ¥éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"âœ… Python è·¯å¾„å·²è®¾ç½®\")\n",
        "print(f\"   é¡¹ç›®æ ¹ç›®å½•ï¼š{PROJECT_ROOT}\")\n",
        "print(f\"   sys.path[0]ï¼š{sys.path[0]}\")\n",
        "\n",
        "# éªŒè¯å¯¼å…¥\n",
        "try:\n",
        "    from src.models.generator import UNetGenerator\n",
        "    from src.models.discriminator import PatchGANDiscriminator\n",
        "    from src.losses.pix2pix_losses import pix2pix_generator_loss\n",
        "    print(\"âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ\")\n",
        "except ImportError as e:\n",
        "    print(f\"âš ï¸  æ¨¡å—å¯¼å…¥å¤±è´¥ï¼š{e}\")\n",
        "    print(\"è¯·æ£€æŸ¥é¡¹ç›®ç»“æ„æ˜¯å¦æ­£ç¡®\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. æ£€æŸ¥ GPU å¯ç”¨æ€§\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"PyTorch ç‰ˆæœ¬ï¼š{torch.__version__}\")\n",
        "print(f\"CUDA å¯ç”¨ï¼š{torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA ç‰ˆæœ¬ï¼š{torch.version.cuda}\")\n",
        "    print(f\"GPU è®¾å¤‡ï¼š{torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU æ•°é‡ï¼š{torch.cuda.device_count()}\")\n",
        "    print(\"âœ… GPU å¯ç”¨ï¼Œè®­ç»ƒå°†ä½¿ç”¨ GPU\")\n",
        "else:\n",
        "    print(\"âš ï¸  GPU ä¸å¯ç”¨ï¼Œè®­ç»ƒå°†ä½¿ç”¨ CPUï¼ˆä¼šå¾ˆæ…¢ï¼‰\")\n",
        "    print(\"å»ºè®®ï¼šåœ¨ Colab ä¸­ç‚¹å‡» Runtime -> Change runtime type -> Hardware accelerator -> GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. è¿è¡Œ E2 å®éªŒï¼šPix2Pix (L1 + GAN)\n",
        "\n",
        "### 8.1 å¿«é€Ÿæµ‹è¯•ï¼ˆ5 epochsï¼Œç”¨äºéªŒè¯ç¯å¢ƒï¼‰\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš ï¸ é‡è¦ï¼šå…è´¹ç‰ˆ Colab é™åˆ¶è¯´æ˜\n",
        "\n",
        "**å…è´¹ç‰ˆ Colab çš„é™åˆ¶ï¼š**\n",
        "- â±ï¸ **è¿è¡Œæ—¶é—´é™åˆ¶**ï¼šæ¯æ¬¡ä¼šè¯æœ€é•¿çº¦ **12 å°æ—¶**ï¼Œä¹‹åä¼šè‡ªåŠ¨æ–­å¼€\n",
        "- ğŸ’¤ **ä¸æ´»åŠ¨æ–­å¼€**ï¼šå¦‚æœçº¦ **90 åˆ†é’Ÿ**æ²¡æœ‰æ“ä½œï¼Œä¼šè¯å¯èƒ½è¢«æ–­å¼€\n",
        "- ğŸ”„ **èµ„æºæŠ¢å **ï¼šå¦‚æœ GPU èµ„æºç´§å¼ ï¼Œå¯èƒ½ä¼šè¢«æŠ¢å \n",
        "\n",
        "**åº”å¯¹ç­–ç•¥ï¼š**\n",
        "1. âœ… **è‡ªåŠ¨ä¿å­˜ checkpoint**ï¼šè®­ç»ƒè„šæœ¬ä¼šæ¯ 10 ä¸ª epoch è‡ªåŠ¨ä¿å­˜ checkpoint\n",
        "2. âœ… **ä¿å­˜åˆ° Google Drive**ï¼šå®šæœŸå°†ç»“æœå¤åˆ¶åˆ° Driveï¼Œé¿å…ä¸¢å¤±\n",
        "3. âœ… **æ¢å¤è®­ç»ƒåŠŸèƒ½**ï¼šå¦‚æœè®­ç»ƒä¸­æ–­ï¼Œå¯ä»¥ä» checkpoint æ¢å¤ç»§ç»­è®­ç»ƒ\n",
        "4. âœ… **åˆ†æ®µè®­ç»ƒ**ï¼šå¯ä»¥å°† 100 epochs åˆ†æˆå¤šæ¬¡è¿è¡Œï¼ˆä¾‹å¦‚ï¼šæ¯æ¬¡ 20-30 epochsï¼‰\n",
        "\n",
        "**å»ºè®®çš„è®­ç»ƒæµç¨‹ï¼š**\n",
        "- ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè®­ç»ƒ 20-30 epochsï¼Œä¿å­˜ checkpoint\n",
        "- å¦‚æœä¸­æ–­ï¼šä½¿ç”¨ `--resume` å‚æ•°ä»æœ€åä¸€ä¸ª checkpoint æ¢å¤\n",
        "- å®šæœŸå¤‡ä»½ï¼šæ¯å®Œæˆä¸€æ®µè®­ç»ƒåï¼Œå°†ç»“æœå¤åˆ¶åˆ° Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¿«é€Ÿæµ‹è¯•ï¼šè¿è¡Œ 5 ä¸ª epoch éªŒè¯ç¯å¢ƒæ˜¯å¦æ­£ç¡®\n",
        "%cd {PROJECT_ROOT}\n",
        "\n",
        "!python src/training/train_pix2pix.py \\\n",
        "  --data-root data \\\n",
        "  --split-index data/splits/cityscapes_split_seed42.json \\\n",
        "  --epochs 5 \\\n",
        "  --batch-size 1 \\\n",
        "  --exp-name pix2pix_l1_gan_strong_debug \\\n",
        "  --aug-mode strong \\\n",
        "  --lambda-l1 100 \\\n",
        "  --num-val-samples 5\n",
        "\n",
        "print(\"\\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 æ­£å¼è®­ç»ƒï¼ˆ200 epochsï¼Œå®Œæ•´ E2 å®éªŒï¼‰\n",
        "\n",
        "**æ³¨æ„**ï¼š100 epochs è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼ˆæ ¹æ® GPU æ€§èƒ½ï¼Œå¯èƒ½éœ€è¦ 3-6 å°æ—¶ï¼‰ã€‚\n",
        "\n",
        "**åˆ†æ®µè®­ç»ƒå»ºè®®ï¼ˆé€‚åˆå…è´¹ç‰ˆ Colabï¼‰ï¼š**\n",
        "- æ–¹æ¡ˆ1ï¼šä¸€æ¬¡æ€§è®­ç»ƒ 100 epochsï¼ˆå¦‚æœæ—¶é—´å…è®¸ï¼‰\n",
        "- æ–¹æ¡ˆ2ï¼šåˆ†æ®µè®­ç»ƒï¼ˆæ¨èï¼‰\n",
        "  - ç¬¬ä¸€æ¬¡ï¼šè®­ç»ƒ 30 epochs\n",
        "  - ç¬¬äºŒæ¬¡ï¼šä» checkpoint æ¢å¤ï¼Œç»§ç»­è®­ç»ƒ 30 epochs\n",
        "  - ç¬¬ä¸‰æ¬¡ï¼šä» checkpoint æ¢å¤ï¼Œç»§ç»­è®­ç»ƒ 40 epochs\n",
        "  - æ¯æ¬¡è®­ç»ƒåè®°å¾—ä¿å­˜åˆ° Drive\n",
        "\n",
        "**å¦‚æœè®­ç»ƒä¸­æ–­ï¼š**\n",
        "- ä½¿ç”¨ä¸‹é¢çš„\"æ¢å¤è®­ç»ƒ\"ä»£ç å—ï¼Œä»æœ€åä¸€ä¸ª checkpoint ç»§ç»­\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ­£å¼ E2 å®éªŒï¼š100 epochsï¼ˆå®Œæ•´è®­ç»ƒï¼‰\n",
        "# å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„\"æ¢å¤è®­ç»ƒ\"ä»£ç å—ç»§ç»­\n",
        "\n",
        "import os\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "!python src/training/train_pix2pix.py \\\n",
        "  --data-root data \\\n",
        "  --split-index data/splits/cityscapes_split_seed42.json \\\n",
        "  --epochs 100 \\\n",
        "  --batch-size 1 \\\n",
        "  --exp-name pix2pix_l1_gan_strong_e100 \\\n",
        "  --aug-mode strong \\\n",
        "  --lambda-l1 100 \\\n",
        "  --lr 2e-4 \\\n",
        "  --start-decay-epoch 50 \\\n",
        "  --num-val-samples 10 \\\n",
        "  --save-interval 10\n",
        "\n",
        "print(\"\\nâœ… E2 å®éªŒè®­ç»ƒå®Œæˆï¼ˆ100 epochsï¼‰\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 æ¢å¤è®­ç»ƒï¼ˆå¦‚æœè®­ç»ƒä¸­æ–­ï¼‰\n",
        "\n",
        "å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªä»£ç å—ä»æœ€åä¸€ä¸ª checkpoint æ¢å¤è®­ç»ƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ¢å¤è®­ç»ƒï¼šä»æœ€åä¸€ä¸ª checkpoint ç»§ç»­\n",
        "# è¿™ä¸ªä»£ç ä¼šè‡ªåŠ¨æ‰¾åˆ°æœ€æ–°çš„ checkpoint å¹¶æ¢å¤è®­ç»ƒ\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "exp_name = \"pix2pix_l1_gan_strong_e100\"\n",
        "checkpoint_dir = PROJECT_ROOT / \"outputs\" / exp_name / \"checkpoints\"\n",
        "\n",
        "# æŸ¥æ‰¾æœ€æ–°çš„ checkpoint\n",
        "checkpoint_files = list(checkpoint_dir.glob(\"checkpoint_epoch_*.pth\"))\n",
        "if checkpoint_files:\n",
        "    # æŒ‰ epoch ç¼–å·æ’åºï¼Œæ‰¾åˆ°æœ€æ–°çš„\n",
        "    checkpoint_files.sort(key=lambda x: int(x.stem.split(\"_\")[-1]))\n",
        "    latest_checkpoint = checkpoint_files[-1]\n",
        "    \n",
        "    print(f\"ğŸ“‚ æ‰¾åˆ°æœ€æ–°çš„ checkpoint: {latest_checkpoint}\")\n",
        "    print(f\"   ä»è¯¥ checkpoint æ¢å¤è®­ç»ƒ...\")\n",
        "    \n",
        "    # ç»§ç»­è®­ç»ƒå‰©ä½™çš„ epochsï¼ˆå‡è®¾æ€»å…±è¦è®­ç»ƒ 100 epochsï¼‰\n",
        "    # è„šæœ¬ä¼šè‡ªåŠ¨ä» checkpoint ä¸­è¯»å–å·²è®­ç»ƒçš„ epoch æ•°\n",
        "    !python src/training/train_pix2pix.py \\\n",
        "      --data-root data \\\n",
        "      --split-index data/splits/cityscapes_split_seed42.json \\\n",
        "      --epochs 100 \\\n",
        "      --batch-size 1 \\\n",
        "      --exp-name {exp_name} \\\n",
        "      --aug-mode strong \\\n",
        "      --lambda-l1 100 \\\n",
        "      --lr 2e-4 \\\n",
        "      --start-decay-epoch 50 \\\n",
        "      --num-val-samples 10 \\\n",
        "      --save-interval 10 \\\n",
        "      --resume {latest_checkpoint}\n",
        "    \n",
        "    print(\"\\nâœ… è®­ç»ƒå·²æ¢å¤å¹¶ç»§ç»­\")\n",
        "else:\n",
        "    print(\"âš ï¸  æœªæ‰¾åˆ° checkpointï¼Œè¯·å…ˆè¿è¡Œåˆå§‹è®­ç»ƒ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4 åˆ†æ®µè®­ç»ƒç¤ºä¾‹ï¼ˆæ¨èï¼šæ¯æ¬¡ 30 epochsï¼‰\n",
        "\n",
        "å¦‚æœæ‹…å¿ƒè®­ç»ƒæ—¶é—´å¤ªé•¿ï¼Œå¯ä»¥åˆ†æ®µè®­ç»ƒï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ†æ®µè®­ç»ƒï¼šç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆepochs 1-30ï¼‰\n",
        "# å®Œæˆåä¿å­˜åˆ° Driveï¼Œä¸‹æ¬¡ä¼šè¯ç»§ç»­è®­ç»ƒ 31-60ï¼Œæœ€åè®­ç»ƒ 61-100\n",
        "\n",
        "import os\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# ç¬¬ä¸€æ¬¡ï¼šè®­ç»ƒå‰ 30 epochs\n",
        "!python src/training/train_pix2pix.py \\\n",
        "  --data-root data \\\n",
        "  --split-index data/splits/cityscapes_split_seed42.json \\\n",
        "  --epochs 30 \\\n",
        "  --batch-size 1 \\\n",
        "  --exp-name pix2pix_l1_gan_strong_e100 \\\n",
        "  --aug-mode strong \\\n",
        "  --lambda-l1 100 \\\n",
        "  --lr 2e-4 \\\n",
        "  --start-decay-epoch 50 \\\n",
        "  --num-val-samples 10 \\\n",
        "  --save-interval 10\n",
        "\n",
        "print(\"\\nâœ… ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå®Œæˆï¼ˆepochs 1-30ï¼‰\")\n",
        "print(\"ğŸ’¾ è¯·è¿è¡Œä¸‹é¢çš„ä»£ç å°†ç»“æœä¿å­˜åˆ° Driveï¼Œç„¶åä¸‹æ¬¡ä¼šè¯ç»§ç»­è®­ç»ƒ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ†æ®µè®­ç»ƒï¼šç¬¬äºŒæ¬¡è¿è¡Œï¼ˆä» epoch 30 ç»§ç»­åˆ° 60ï¼‰\n",
        "# ä½¿ç”¨ä¸Šé¢çš„\"æ¢å¤è®­ç»ƒ\"ä»£ç å—ï¼Œä½†ä¿®æ”¹ epochs å‚æ•°\n",
        "# æˆ–è€…ç›´æ¥è¿è¡Œï¼š\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "exp_name = \"pix2pix_l1_gan_strong_e100\"\n",
        "checkpoint_dir = PROJECT_ROOT / \"outputs\" / exp_name / \"checkpoints\"\n",
        "latest_checkpoint = sorted(checkpoint_dir.glob(\"checkpoint_epoch_*.pth\"), \n",
        "                           key=lambda x: int(x.stem.split(\"_\")[-1]))[-1]\n",
        "\n",
        "print(f\"ğŸ“‚ ä» checkpoint æ¢å¤: {latest_checkpoint}\")\n",
        "\n",
        "!python src/training/train_pix2pix.py \\\n",
        "  --data-root data \\\n",
        "  --split-index data/splits/cityscapes_split_seed42.json \\\n",
        "  --epochs 60 \\\n",
        "  --batch-size 1 \\\n",
        "  --exp-name {exp_name} \\\n",
        "  --aug-mode strong \\\n",
        "  --lambda-l1 100 \\\n",
        "  --lr 2e-4 \\\n",
        "  --start-decay-epoch 50 \\\n",
        "  --num-val-samples 10 \\\n",
        "  --save-interval 10 \\\n",
        "  --resume {latest_checkpoint}\n",
        "\n",
        "print(\"\\nâœ… ç¬¬äºŒé˜¶æ®µè®­ç»ƒå®Œæˆï¼ˆepochs 31-60ï¼‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ†æ®µè®­ç»ƒï¼šç¬¬ä¸‰æ¬¡è¿è¡Œï¼ˆä» epoch 60 ç»§ç»­åˆ° 100ï¼Œå®Œæˆè®­ç»ƒï¼‰\n",
        "# ä½¿ç”¨ä¸Šé¢çš„\"æ¢å¤è®­ç»ƒ\"ä»£ç å—ï¼Œä¿®æ”¹ epochs ä¸º 100\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "exp_name = \"pix2pix_l1_gan_strong_e100\"\n",
        "checkpoint_dir = PROJECT_ROOT / \"outputs\" / exp_name / \"checkpoints\"\n",
        "latest_checkpoint = sorted(checkpoint_dir.glob(\"checkpoint_epoch_*.pth\"), \n",
        "                           key=lambda x: int(x.stem.split(\"_\")[-1]))[-1]\n",
        "\n",
        "print(f\"ğŸ“‚ ä» checkpoint æ¢å¤: {latest_checkpoint}\")\n",
        "\n",
        "!python src/training/train_pix2pix.py \\\n",
        "  --data-root data \\\n",
        "  --split-index data/splits/cityscapes_split_seed42.json \\\n",
        "  --epochs 100 \\\n",
        "  --batch-size 1 \\\n",
        "  --exp-name {exp_name} \\\n",
        "  --aug-mode strong \\\n",
        "  --lambda-l1 100 \\\n",
        "  --lr 2e-4 \\\n",
        "  --start-decay-epoch 50 \\\n",
        "  --num-val-samples 10 \\\n",
        "  --save-interval 10 \\\n",
        "  --resume {latest_checkpoint}\n",
        "\n",
        "print(\"\\nâœ… ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒå®Œæˆï¼ˆepochs 61-100ï¼‰\")\n",
        "print(\"ğŸ‰ å®Œæ•´è®­ç»ƒå·²å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ä¿å­˜ç»“æœåˆ° Google Driveï¼ˆé‡è¦ï¼šé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰\n",
        "\n",
        "**å»ºè®®ï¼š**\n",
        "- æ¯æ¬¡è®­ç»ƒå®Œæˆåï¼Œç«‹å³è¿è¡Œä¸‹é¢çš„ä»£ç ä¿å­˜åˆ° Drive\n",
        "- å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œcheckpoint å·²ç»ä¿å­˜åœ¨æœ¬åœ°ï¼Œå¯ä»¥æ¢å¤è®­ç»ƒåå†ä¿å­˜\n",
        "- ä¿å­˜åˆ° Drive åï¼Œä¸‹æ¬¡ä¼šè¯å¯ä»¥ä» Drive æ¢å¤ checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å°†è®­ç»ƒç»“æœï¼ˆcheckpointsã€imagesã€logsï¼‰å¤åˆ¶åˆ° Google Drive\n",
        "# è¿™æ ·å³ä½¿ Colab ä¼šè¯ç»“æŸï¼Œç»“æœä¹Ÿä¸ä¼šä¸¢å¤±\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "outputs_dir = PROJECT_ROOT / \"outputs\"\n",
        "drive_outputs_dir = Path(\"/content/drive/MyDrive/Cityscapes/Image-to-Image-Translation-Experiment/outputs\")\n",
        "\n",
        "if outputs_dir.exists():\n",
        "    # åˆ›å»º Drive è¾“å‡ºç›®å½•\n",
        "    drive_outputs_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"ğŸ“¦ æ­£åœ¨å¤åˆ¶ç»“æœåˆ° Driveï¼š{drive_outputs_dir}\")\n",
        "    \n",
        "    # é€ä¸ªå¤åˆ¶å®éªŒç›®å½•\n",
        "    for exp_dir in outputs_dir.iterdir():\n",
        "        if exp_dir.is_dir():\n",
        "            dest_dir = drive_outputs_dir / exp_dir.name\n",
        "            \n",
        "            # å¦‚æœç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œå¢é‡æ›´æ–°ï¼ˆåªå¤åˆ¶æ–°æ–‡ä»¶ï¼‰\n",
        "            if dest_dir.exists():\n",
        "                print(f\"  ğŸ”„ æ›´æ–°å·²å­˜åœ¨çš„ç›®å½•ï¼š{exp_dir.name}\")\n",
        "                # å¤åˆ¶ checkpointsï¼ˆå¢é‡ï¼‰\n",
        "                if (exp_dir / \"checkpoints\").exists():\n",
        "                    (dest_dir / \"checkpoints\").mkdir(exist_ok=True, parents=True)\n",
        "                    for ckpt in (exp_dir / \"checkpoints\").glob(\"*.pth\"):\n",
        "                        dest_ckpt = dest_dir / \"checkpoints\" / ckpt.name\n",
        "                        if not dest_ckpt.exists() or ckpt.stat().st_mtime > dest_ckpt.stat().st_mtime:\n",
        "                            shutil.copy2(ckpt, dest_ckpt)\n",
        "                            print(f\"    âœ… å·²å¤åˆ¶ checkpoint: {ckpt.name}\")\n",
        "                \n",
        "                # å¤åˆ¶ imagesï¼ˆå¢é‡ï¼‰\n",
        "                if (exp_dir / \"images\").exists():\n",
        "                    (dest_dir / \"images\").mkdir(exist_ok=True, parents=True)\n",
        "                    for img in (exp_dir / \"images\").glob(\"*.png\"):\n",
        "                        dest_img = dest_dir / \"images\" / img.name\n",
        "                        if not dest_img.exists() or img.stat().st_mtime > dest_img.stat().st_mtime:\n",
        "                            shutil.copy2(img, dest_img)\n",
        "                \n",
        "                # å¤åˆ¶ logsï¼ˆè¦†ç›–ï¼‰\n",
        "                if (exp_dir / \"logs\").exists():\n",
        "                    shutil.copytree(exp_dir / \"logs\", dest_dir / \"logs\", dirs_exist_ok=True)\n",
        "            else:\n",
        "                # å…¨æ–°å¤åˆ¶\n",
        "                shutil.copytree(exp_dir, dest_dir)\n",
        "                print(f\"  âœ… å·²å¤åˆ¶æ–°ç›®å½•ï¼š{exp_dir.name}\")\n",
        "    \n",
        "    print(f\"\\nâœ… ç»“æœå·²ä¿å­˜åˆ° Driveï¼š{drive_outputs_dir}\")\n",
        "    print(f\"ğŸ’¡ ä¸‹æ¬¡ä¼šè¯å¯ä»¥ä» Drive æ¢å¤ checkpoint ç»§ç»­è®­ç»ƒ\")\n",
        "else:\n",
        "    print(f\"âš ï¸  è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼š{outputs_dir}\")\n",
        "    print(\"è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 ä» Google Drive æ¢å¤ checkpointï¼ˆå¦‚æœæœ¬åœ°æ–‡ä»¶ä¸¢å¤±ï¼‰\n",
        "\n",
        "å¦‚æœ Colab ä¼šè¯æ–­å¼€åé‡æ–°å¼€å§‹ï¼Œå¯ä»¥ä» Drive æ¢å¤ checkpointï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä» Google Drive æ¢å¤ checkpoint åˆ°æœ¬åœ°\n",
        "# å¦‚æœæœ¬åœ° outputs ç›®å½•è¢«æ¸…ç©ºï¼Œå¯ä»¥ä» Drive æ¢å¤\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "drive_outputs_dir = Path(\"/content/drive/MyDrive/Cityscapes/Image-to-Image-Translation-Experiment/outputs\")\n",
        "local_outputs_dir = PROJECT_ROOT / \"outputs\"\n",
        "\n",
        "exp_name = \"pix2pix_l1_gan_strong_e100\"\n",
        "drive_exp_dir = drive_outputs_dir / exp_name\n",
        "local_exp_dir = local_outputs_dir / exp_name\n",
        "\n",
        "if drive_exp_dir.exists():\n",
        "    print(f\"ğŸ“¥ ä» Drive æ¢å¤ï¼š{drive_exp_dir}\")\n",
        "    \n",
        "    # åˆ›å»ºæœ¬åœ°ç›®å½•\n",
        "    local_exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # æ¢å¤ checkpoints\n",
        "    if (drive_exp_dir / \"checkpoints\").exists():\n",
        "        (local_exp_dir / \"checkpoints\").mkdir(exist_ok=True, parents=True)\n",
        "        for ckpt in (drive_exp_dir / \"checkpoints\").glob(\"*.pth\"):\n",
        "            shutil.copy2(ckpt, local_exp_dir / \"checkpoints\" / ckpt.name)\n",
        "            print(f\"  âœ… å·²æ¢å¤ checkpoint: {ckpt.name}\")\n",
        "    \n",
        "    # æ¢å¤ logs\n",
        "    if (drive_exp_dir / \"logs\").exists():\n",
        "        shutil.copytree(drive_exp_dir / \"logs\", local_exp_dir / \"logs\", dirs_exist_ok=True)\n",
        "        print(f\"  âœ… å·²æ¢å¤ logs\")\n",
        "    \n",
        "    print(f\"\\nâœ… å·²ä» Drive æ¢å¤åˆ°æœ¬åœ°ï¼š{local_exp_dir}\")\n",
        "    print(f\"ğŸ’¡ ç°åœ¨å¯ä»¥ä½¿ç”¨ä¸Šé¢çš„'æ¢å¤è®­ç»ƒ'ä»£ç å—ç»§ç»­è®­ç»ƒ\")\n",
        "else:\n",
        "    print(f\"âš ï¸  Drive ä¸­æœªæ‰¾åˆ°å®éªŒç›®å½•ï¼š{drive_exp_dir}\")\n",
        "    print(\"è¯·å…ˆè¿è¡Œè®­ç»ƒå¹¶ä¿å­˜åˆ° Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹è®­ç»ƒå†å²ï¼ˆloss æ›²çº¿ç­‰ï¼‰\n",
        "import json\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_file = PROJECT_ROOT / \"outputs\" / \"pix2pix_l1_gan_strong\" / \"logs\" / \"history_pix2pix.json\"\n",
        "\n",
        "if history_file.exists():\n",
        "    with open(history_file, 'r', encoding='utf-8') as f:\n",
        "        history = json.load(f)\n",
        "    \n",
        "    print(\"è®­ç»ƒå†å²æ‘˜è¦ï¼š\")\n",
        "    print(f\"  æ€» epoch æ•°ï¼š{len(history.get('train_g_total', []))}\")\n",
        "    if history.get('val_psnr'):\n",
        "        print(f\"  æœ€ä½³ Val PSNRï¼š{max(history.get('val_psnr', [0])):.4f}\")\n",
        "    if history.get('val_ssim'):\n",
        "        print(f\"  æœ€ä½³ Val SSIMï¼š{max(history.get('val_ssim', [0])):.4f}\")\n",
        "    if history.get('val_l1'):\n",
        "        print(f\"  æœ€ä½ Val L1ï¼š{min(history.get('val_l1', [float('inf')])):.4f}\")\n",
        "    \n",
        "    # ç»˜åˆ¶æŸå¤±æ›²çº¿ï¼ˆç®€å•ç¤ºä¾‹ï¼‰\n",
        "    epochs = list(range(1, len(history.get('train_g_total', [])) + 1))\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    if history.get('train_g_total'):\n",
        "        plt.plot(epochs, history.get('train_g_total', []), label='G Total')\n",
        "    if history.get('train_g_l1'):\n",
        "        plt.plot(epochs, history.get('train_g_l1', []), label='G L1')\n",
        "    if history.get('train_g_gan'):\n",
        "        plt.plot(epochs, history.get('train_g_gan', []), label='G GAN')\n",
        "    if history.get('train_d_loss'):\n",
        "        plt.plot(epochs, history.get('train_d_loss', []), label='D Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Losses')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    if history.get('val_psnr'):\n",
        "        plt.plot(epochs, history.get('val_psnr', []), label='PSNR')\n",
        "    if history.get('val_ssim'):\n",
        "        plt.plot(epochs, history.get('val_ssim', []), label='SSIM')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.title('Validation Metrics')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"âš ï¸  è®­ç»ƒå†å²æ–‡ä»¶ä¸å­˜åœ¨ï¼š{history_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
