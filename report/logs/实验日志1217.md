# 实验日志 2025-12-17（U-Net 基线 & 数据增强消融）

## 背景
- 目标：基于 Cityscapes 数据，完成 U-Net 基线模型训练，并按照《ABLATION_GUIDE.md》设计的三种数据增强配置（none / basic / strong）做消融实验。
- 数据来源：`data/processed/{train,val}/{photo,label}`，划分索引 `data/splits/cityscapes_split_seed42.json`。
- 模型：`src/models/unet_baseline.py` 中的 `UNetBaseline`，输入 label，输出 photo，Tanh 输出到 \([-1,1]\)。

## 操作记录

1. 环境与依赖检查
   - 在 `01_unet_baseline.ipynb` 中完成本地环境设置：切到项目根目录，检测 PyTorch 与 GPU 状态。
   - 校验关键文件存在：`unet_baseline.py`、`dataset.py`、`transforms.py`、`cityscapes_split_seed42.json`。

2. 模型结构验证
   - 构建 `UNetBaseline(in_channels=3, out_channels=3)`，在 GPU 上用 `1×3×256×256` 随机输入做前向测试。
   - 确认输出尺寸为 `1×3×256×256`，输出范围约 `[-0.99, 0.99]`，参数量约 5,441 万。

3. 数据增强配置与可视化
   - 基于 `build_transform` 定义三种增强（photo 采用 `normalize_mode="tanh"`）：
     - none：仅 `resize(256)`，无 jitter、无翻转、无颜色/尺度扰动。
     - basic：`resize(256)` + 随机 jitter + 随机水平翻转。
     - strong：在 basic 基础上增加 `color_jitter=(0.2,0.2,0.2,0.05)` 与 `scale_range=(0.8,1.2)`。
   - 验证集统一使用无增强版本（只做尺寸/归一化），保证评估分布一致。
   - 抽取 1 个 `val` 样本，对三种增强下的 label/photo 做可视化对比，保存到 `outputs/figures/unet_baseline/augmentation_comparison.png`。

4. 训练与验证流程（基线 + 增强消融）
   - 统一训练配置：
     - epochs=200，batch_size=16，优化器 `Adam(lr=2e-4, betas=(0.5,0.999))`。
     - 学习率：`epoch>=100` 后线性衰减（LambdaLR）。
     - 损失函数：`nn.L1Loss()`，仅对生成 photo 与真实 photo 做像素级 L1。
   - 数据加载：
     - 训练：`CityscapesDataset(split="train", transform=aug_configs[aug_mode])`。
     - 验证：`CityscapesDataset(split="val", transform=val_transform)`。
   - 每个 epoch：
     - `train_epoch` 完成一轮训练，返回平均 L1 loss。
     - `validate` 在 val 上评估，计算平均 Val L1 / PSNR / SSIM / MAE（调用 `src/eval/metrics.py` 中的 `evaluate_batch`），并保存若干三联图（`label / generated / ground_truth`）到 `outputs/images/unet_baseline/`，文件名带 `aug_mode` 后缀。

5. 三种增强模式的具体训练
   - 无增强 baseline（`aug_mode="none"`）：
     - 使用 `aug_configs["none"]` 训练 200 epoch。
     - 每 10 个 epoch 打印 Train/Val loss 和 PSNR/SSIM/MAE、当前 LR，并保存 `checkpoint_none_epoch_XXX.pth`。
     - 结束后保存：`model_none_final.pth`、`history_none.json`、`sample_*_none.png`。
   - 基础增强（`aug_mode="basic"`）：
     - 修改为 `aug_mode="basic"`，重新构建数据集和模型，完整训练 200 epoch。
     - 保存：`model_basic_final.pth`、`history_basic.json`、`sample_*_basic.png` 及对应 checkpoints。
   - 强增强（`aug_mode="strong"`）：
     - 修改为 `aug_mode="strong"`，同样训练 200 epoch。
     - 保存：`model_strong_final.pth`、`history_strong.json`、`sample_*_strong.png` 及对应 checkpoints。

6. 消融结果汇总与可视化
   - 加载 `history_none.json / history_basic.json / history_strong.json` 形成 `histories` 字典。
   - 生成对比可视化（位于 `outputs/figures/unet_baseline/`）：
     - `training_curve_{aug_mode}.png`：单配置 Train/Val loss 曲线。
     - `ablation_comparison_curves.png`：三种配置的损失和 PSNR/SSIM 曲线对比。
     - `ablation_metrics_comparison.png`：三种配置最终 Val PSNR / SSIM / MAE 柱状图。
   - 汇总表：
     - 用 pandas 汇总每种配置最后一个 epoch 的 Val Loss / PSNR / SSIM / MAE 及最佳 PSNR/SSIM 和对应 epoch，保存为 `ablation_results.csv`。
   - 额外可视化（如需）：
     - 生成 `ablation_samples_comparison.png`，对比不同增强下的生成结果；可选生成文字版 `ablation_report.txt` 做简要分析。

## 结果摘要
- 数据增强：实现并验证了 none / basic / strong 三档增强策略，验证集统一使用无增强配置评估。
- 基线训练：对每种增强配置均完成一次 200 epoch 的 U-Net baseline 训练，记录全程 L1 / PSNR / SSIM / MAE。
- 消融实验：基于 `history_{aug_mode}.json`、多种曲线图和样例图，得到"增强强度 vs 性能/稳定性"的完整对比结果，为后续报告撰写和结论给出了量化与可视化依据。

---

## Pix2Pix 模型架构实现（Guide.md 7.1）

### 7.1.1 U-Net生成器实现
- **文件**：`src/models/generator.py`
- **架构**：按照 Guide.md 7.1.1 要求实现
  - 编码器：C64-C128-C256-C512-C512-C512-C512-C512
  - 解码器：CD512-CD512-CD512-C512-C256-C128-C64
  - Skip Connections：编码器第i层连接到解码器第n-i层
- **验证结果**：
  - 输入：`[B, 3, 256, 256]` (label图像)
  - 输出：`[B, 3, 256, 256]` (生成的photo图像，范围[-1, 1])
  - 参数量：54,414,019
  - 前向传播测试通过

### 7.1.2 PatchGAN判别器实现
- **文件**：`src/models/discriminator.py`
- **架构**：按照 Guide.md 7.1.2 要求实现
  - 70×70 PatchGAN：C64-C128-C256-C512
  - 输入：concatenate(label, image) = 6通道
  - 输出：15×15 的 patch 判别结果（每个 patch 对应输入中约 70×70 的感受野）
- **验证结果**：
  - 输入：`label [B, 3, 256, 256]` + `image [B, 3, 256, 256]`
  - 输出：`[B, 1, 15, 15]` (patch判别结果，范围[0, 1])
  - 参数量：2,768,705
  - 前向传播测试通过

### 联合测试
- **文件**：`src/models/test_pix2pix_models.py`
- 生成器和判别器联合测试通过，两个模型可以正常配合使用

---

## Pix2Pix 损失函数实现（Guide.md 7.2）

### 基础损失函数实现
- **文件**：
  - `src/losses/adversarial_loss.py` - 对抗损失（GAN损失）
  - `src/losses/pix2pix_losses.py` - 组合损失工具模块

### 7.2.1.1 L1损失
- **实现**：`l1_loss(predicted, target)`
- **公式**：`L_L1 = ||G(x) - y||_1`
- **用途**：像素级重建损失，确保生成图像与真实图像在像素级别接近

### 7.2.1.2 对抗损失（GAN损失）
- **生成器对抗损失**：
  - 函数：`generator_adversarial_loss(discriminator_output, target_is_real=True)`
  - 公式：`L_GAN = log(D(x, G(x)))`
  - 实现：使用 `BCELoss`，希望判别器认为生成图像是真实的（target=1）
  - 支持 logits 版本：`generator_adversarial_loss_with_logits()`（使用 `BCEWithLogitsLoss`）

- **判别器对抗损失**：
  - 函数：`discriminator_adversarial_loss(discriminator_real, discriminator_fake)`
  - 公式：`L_D = -[log(D(x, y)) + log(1 - D(x, G(x)))]`
  - 实现：使用 `BCELoss`，分别对真实图像（target=1）和生成图像（target=0）计算损失
  - 支持 logits 版本：`discriminator_adversarial_loss_with_logits()`

### 7.2.1.3 组合损失
- **生成器总损失**：
  - 函数：`pix2pix_generator_loss(fake_image, real_image, discriminator_output, lambda_l1=100.0)`
  - 公式：`L_total = L_GAN + λ * L_L1`（λ通常为100）
  - 返回：总损失、GAN损失、L1损失

- **判别器总损失**：
  - 函数：`pix2pix_discriminator_loss(discriminator_real_output, discriminator_fake_output)`
  - 返回：判别器对抗损失

### 测试验证（基础损失）
- 所有损失函数测试通过
- 损失值计算正确，符合预期范围
- 支持批量计算和不同输入尺寸

### 7.2.2 扩展损失（消融实验）

#### 7.2.2.1 Feature Matching Loss
- **文件**：`src/losses/feature_matching.py`
- **实现思路**：
  - 从判别器中提取多层中间特征：`{f_l(real), f_l(fake)}`
  - 支持两种输入形式：`List[Tensor]` 或 `Dict[str, Tensor]`
  - 对每一层分别计算 L1 损失 `||f_l(real) - f_l(fake)||_1`，再在所有层上取平均：
    - `L_FM = (1 / L) * Σ_l ||f_l(real) - f_l(fake)||_1`
- **测试**：
  - 使用随机张量构造 3 层特征，分别以列表和字典形式传入
  - List 和 Dict 两种形式的损失值一致，测试通过

#### 7.2.2.2 Perceptual Loss
- **文件**：`src/losses/perceptual_loss.py`
- **实现思路**：
  - 使用 `torchvision.models.vgg16` 作为特征提取 backbone（封装为 `VGGFeatureExtractor`）
  - 默认提取多层特征（大致对应 `relu1_2, relu2_2, relu3_3, relu4_3`）
  - 输入假定来自 Pix2Pix 生成器，范围为 `[-1, 1]`，先映射到 `[0, 1]`，再按 ImageNet 统计量归一化：
    - `x_01 = (x + 1) / 2`
    - `x_norm = (x_01 - mean) / std`
  - 对每一层特征计算 L1 损失并平均，得到感知损失：
    - `L_perc = (1 / L) * Σ_l ||ϕ_l(G(x)) - ϕ_l(y)||_1`
  - 提供 `PerceptualLoss(pretrained=False)` 接口，测试时不强制下载预训练权重；实际训练中可将 `pretrained=True` 以使用 ImageNet 预训练 VGG
- **测试**：
  - 使用随机输入 `x` 及其轻微扰动版本 `y = x + 0.05 * noise`，验证感知损失为一个合理的正数

### 测试验证（扩展损失）
- Feature Matching Loss 与 Perceptual Loss 的单元测试均通过
- 两种扩展损失均支持 batch 输入和 Cityscapes 分辨率（256×256）
- 后续将在 Pix2Pix 训练脚本中接入，用于完成损失函数消融实验（L1 vs L1+GAN vs L1+GAN+FM vs L1+GAN+FM+Perceptual）
