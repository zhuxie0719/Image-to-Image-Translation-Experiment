{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 CycleGANè®­ç»ƒå®éªŒï¼ˆColabç‰ˆæœ¬ï¼‰\n",
        "\n",
        "- ä»GitHubä»“åº“æ‹‰å–ä»£ç \n",
        "- CycleGANæ¨¡å‹è®­ç»ƒï¼ˆLabel â†” PhotoåŒå‘è½¬æ¢ï¼‰\n",
        "- å¾ªç¯ä¸€è‡´æ€§æŸå¤±\n",
        "- é€‚é…Google Colabç¯å¢ƒ\n",
        "\n",
        "## ä½¿ç”¨è¯´æ˜\n",
        "\n",
        "1. åœ¨Colabä¸­æ‰“å¼€æ­¤notebook\n",
        "2. ç¡®ä¿å·²å¯ç”¨GPUè¿è¡Œæ—¶ï¼ˆè¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPUï¼‰\n",
        "3. æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰å•å…ƒæ ¼\n",
        "4. å¦‚æœéœ€è¦ä¸Šä¼ æ•°æ®ï¼Œè¯·æŒ‰ç…§æç¤ºæ“ä½œ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ç¯å¢ƒè®¾ç½®å’ŒGitHubä»“åº“å…‹éš†\n",
        "\n",
        "**é‡è¦æç¤º**ï¼š\n",
        "- Checkpointæ–‡ä»¶è¾ƒå¤§ï¼Œå»ºè®®ä¿å­˜åˆ°Google Drive\n",
        "- ä»£ç å·²é…ç½®ä¸ºè‡ªåŠ¨å°†checkpointä¿å­˜åˆ°Drive\n",
        "- æ¢å¤è®­ç»ƒæ—¶ï¼Œåªéœ€è®¾ç½® `RESUME_EPOCH` å‚æ•°å³å¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Colabç¯å¢ƒè®¾ç½® - ä»GitHubå…‹éš†ä»“åº“\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# æ£€æµ‹æ˜¯å¦åœ¨Colabç¯å¢ƒä¸­\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"âœ… æ£€æµ‹åˆ°Colabç¯å¢ƒ\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"âš ï¸  æœªæ£€æµ‹åˆ°Colabç¯å¢ƒï¼Œå°†ä½¿ç”¨æœ¬åœ°è·¯å¾„\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    # ============================================\n",
        "    # æ–¹æ³•1ï¼šä»GitHubå…‹éš†ä»“åº“ï¼ˆæ¨èï¼‰\n",
        "    # ============================================\n",
        "    # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…GitHubä»“åº“åœ°å€\n",
        "    GITHUB_REPO_URL = \"https://github.com/AltheD/Image-to-Image-Translation-Experiment\"\n",
        "    REPO_NAME = \"Image-to-Image-Translation-Experiment\"\n",
        "    \n",
        "    # å¦‚æœä»“åº“æ˜¯ç§æœ‰çš„ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆéœ€è¦è®¾ç½®GitHub tokenï¼‰ï¼š\n",
        "    # GITHUB_REPO_URL = \"https://YOUR_TOKEN@github.com/YOUR_USERNAME/Image-to-Image-Translation-Experiment.git\"\n",
        "    \n",
        "    USE_GITHUB_CLONE = True  # è®¾ç½®ä¸ºTrueä»¥å¯ç”¨GitHubå…‹éš†\n",
        "    \n",
        "    if USE_GITHUB_CLONE and GITHUB_REPO_URL != \"https://github.com/YOUR_USERNAME/Image-to-Image-Translation-Experiment.git\":\n",
        "        print(\"æ­£åœ¨ä»GitHubå…‹éš†ä»“åº“...\")\n",
        "        repo_path = Path(f\"/content/{REPO_NAME}\")\n",
        "        if not repo_path.exists():\n",
        "            try:\n",
        "                subprocess.run([\"git\", \"clone\", GITHUB_REPO_URL, str(repo_path)], check=True)\n",
        "                print(f\"âœ… ä»“åº“å…‹éš†å®Œæˆ: {repo_path}\")\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"âŒ å…‹éš†å¤±è´¥: {e}\")\n",
        "                print(\"   è¯·æ£€æŸ¥ï¼š\")\n",
        "                print(\"   1. GitHubä»“åº“URLæ˜¯å¦æ­£ç¡®\")\n",
        "                print(\"   2. ä»“åº“æ˜¯å¦ä¸ºå…¬å¼€ï¼Œæˆ–å·²é…ç½®è®¿é—®æƒé™\")\n",
        "                print(\"   3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\")\n",
        "                USE_GITHUB_CLONE = False\n",
        "        else:\n",
        "            print(f\"âœ… ä»“åº“å·²å­˜åœ¨: {repo_path}\")\n",
        "            # å¯é€‰ï¼šæ›´æ–°ä»“åº“\n",
        "            # subprocess.run([\"git\", \"-C\", str(repo_path), \"pull\"], check=False)\n",
        "        \n",
        "        if USE_GITHUB_CLONE:\n",
        "            ROOT = repo_path\n",
        "        else:\n",
        "            ROOT = Path(\"/content\")\n",
        "    else:\n",
        "        # ============================================\n",
        "        # æ–¹æ³•2ï¼šç›´æ¥ä½¿ç”¨ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆå¦‚æœnotebookå·²ä¸Šä¼ åˆ°Colabï¼‰\n",
        "        # ============================================\n",
        "        ROOT = Path(\"/content\")\n",
        "        print(\"âš ï¸  æœªé…ç½®GitHubä»“åº“URLï¼Œä½¿ç”¨/contentç›®å½•\")\n",
        "        print(\"   è¯·ç¡®ä¿é¡¹ç›®æ–‡ä»¶å·²ä¸Šä¼ åˆ°Colabï¼Œæˆ–ä¿®æ”¹ä¸Šé¢çš„GITHUB_REPO_URL\")\n",
        "    \n",
        "    # åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•\n",
        "    os.chdir(ROOT)\n",
        "    \n",
        "    print(f\"\\nâœ… Colabç¯å¢ƒè®¾ç½®å®Œæˆï¼\")\n",
        "    print(f\"é¡¹ç›®æ ¹ç›®å½•ï¼š{ROOT}\")\n",
        "    print(f\"å½“å‰å·¥ä½œç›®å½•ï¼š{os.getcwd()}\")\n",
        "    \n",
        "    # æŒ‚è½½Google Driveï¼ˆå¯é€‰ï¼Œç”¨äºä¿å­˜ç»“æœå’Œæ•°æ®ï¼‰\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"âœ… Google Driveå·²æŒ‚è½½\")\n",
        "        DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Google DriveæŒ‚è½½å¤±è´¥: {e}\")\n",
        "        DRIVE_ROOT = None\n",
        "else:\n",
        "    # æœ¬åœ°ç¯å¢ƒ\n",
        "    ROOT = Path(\"..\").resolve()\n",
        "    os.chdir(ROOT)\n",
        "    print(f\"âœ… æœ¬åœ°ç¯å¢ƒè®¾ç½®å®Œæˆï¼\")\n",
        "    print(f\"é¡¹ç›®æ ¹ç›®å½•ï¼š{ROOT}\")\n",
        "    DRIVE_ROOT = None\n",
        "\n",
        "# ä¿å­˜ROOTè·¯å¾„ä¾›åç»­ä½¿ç”¨\n",
        "COLAB_WORK_DIR = ROOT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# å®‰è£…ä¾èµ–åŒ…\n",
        "# ============================================\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"æ­£åœ¨å®‰è£…ä¾èµ–åŒ…...\")\n",
        "    \n",
        "    # å®‰è£…å¿…è¦çš„PythonåŒ…\n",
        "    packages = [\n",
        "        \"torch\",\n",
        "        \"torchvision\",\n",
        "        \"tqdm\",\n",
        "        \"Pillow\",\n",
        "        \"scikit-image\",  # ç”¨äºSSIMè®¡ç®—\n",
        "    ]\n",
        "    \n",
        "    for package in packages:\n",
        "        print(f\"å®‰è£… {package}...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package], check=False)\n",
        "    \n",
        "    print(\"âœ… ä¾èµ–åŒ…å®‰è£…å®Œæˆï¼\")\n",
        "else:\n",
        "    print(\"æœ¬åœ°ç¯å¢ƒï¼Œè·³è¿‡ä¾èµ–å®‰è£…\")\n",
        "\n",
        "# æ£€æŸ¥GPUå¯ç”¨æ€§\n",
        "print(\"\\næ­£åœ¨æ£€æŸ¥GPU...\")\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
        "    \n",
        "    cuda_available = torch.cuda.is_available()\n",
        "    if cuda_available:\n",
        "        print(f\"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
        "        print(f\"   GPUæ•°é‡: {torch.cuda.device_count()}\")\n",
        "        print(f\"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    else:\n",
        "        print(\"âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè¿è¡Œï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰\")\n",
        "        print(\"   è¯·åœ¨Colabä¸­å¯ç”¨GPUï¼šè¿è¡Œæ—¶ â†’ æ›´æ”¹è¿è¡Œæ—¶ç±»å‹ â†’ GPU\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸  è­¦å‘Šï¼štorchæœªå®‰è£…\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  æ£€æŸ¥GPUæ—¶å‡ºé”™: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# éªŒè¯é¡¹ç›®æ–‡ä»¶ç»“æ„\n",
        "# ============================================\n",
        "\n",
        "print(\"ğŸ“‹ éªŒè¯é¡¹ç›®æ–‡ä»¶...\")\n",
        "required_files = [\n",
        "    (\"src/models/cyclegan_generator.py\", \"CycleGANç”Ÿæˆå™¨\"),\n",
        "    (\"src/models/cyclegan_discriminator.py\", \"CycleGANåˆ¤åˆ«å™¨\"),\n",
        "    (\"src/data/dataset.py\", \"æ•°æ®é›†æ–‡ä»¶\"),\n",
        "    (\"src/data/transforms.py\", \"æ•°æ®å˜æ¢æ–‡ä»¶\"),\n",
        "    (\"src/eval/metrics.py\", \"è¯„ä¼°æŒ‡æ ‡\"),\n",
        "    (\"data/splits/cityscapes_split_seed42.json\", \"æ•°æ®åˆ’åˆ†æ–‡ä»¶\"),\n",
        "]\n",
        "\n",
        "all_ok = True\n",
        "for file_path, description in required_files:\n",
        "    full_path = ROOT / file_path\n",
        "    if full_path.exists():\n",
        "        print(f\"âœ… {description}: {file_path}\")\n",
        "    else:\n",
        "        print(f\"âŒ {description} ç¼ºå¤±: {file_path}\")\n",
        "        all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\nğŸ‰ æ‰€æœ‰å¿…éœ€æ–‡ä»¶å·²å°±ç»ªï¼\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥é¡¹ç›®ç»“æ„\")\n",
        "    print(\"   å¦‚æœä½¿ç”¨GitHubå…‹éš†ï¼Œè¯·ç¡®ä¿ä»“åº“å®Œæ•´\")\n",
        "    print(\"   å¦‚æœç›´æ¥ä¸Šä¼ ï¼Œè¯·ç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½å·²ä¸Šä¼ åˆ°Colab\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. æ•°æ®å‡†å¤‡\n",
        "\n",
        "**é‡è¦æç¤º**ï¼šCityscapesæ•°æ®é›†è¾ƒå¤§ï¼Œéœ€è¦æ‰‹åŠ¨å‡†å¤‡ï¼š\n",
        "\n",
        "1. **é€‰é¡¹Aï¼ˆæ¨èï¼‰**ï¼šå¦‚æœæ•°æ®å·²åœ¨Google Driveä¸­\n",
        "   - å°†æ•°æ®æ–‡ä»¶å¤¹ä¸Šä¼ åˆ°Google Drive\n",
        "   - åœ¨ä¸‹é¢çš„ä»£ç ä¸­æŒ‡å®šDriveä¸­çš„è·¯å¾„\n",
        "\n",
        "2. **é€‰é¡¹B**ï¼šå¦‚æœæ•°æ®åœ¨GitHubä»“åº“ä¸­\n",
        "   - ç¡®ä¿æ•°æ®å·²åŒ…å«åœ¨ä»“åº“ä¸­ï¼ˆå¯èƒ½éœ€è¦ä½¿ç”¨Git LFSï¼‰\n",
        "\n",
        "3. **é€‰é¡¹C**ï¼šåœ¨Colabä¸­ä¸‹è½½æ•°æ®\n",
        "   - ä½¿ç”¨wgetæˆ–å…¶ä»–å·¥å…·ä¸‹è½½æ•°æ®é›†\n",
        "   - è§£å‹åˆ°æŒ‡å®šç›®å½•\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# æ•°æ®è·¯å¾„è®¾ç½®\n",
        "# ============================================\n",
        "\n",
        "# æ–¹æ³•1ï¼šä»Google DriveåŠ è½½æ•°æ®ï¼ˆæ¨èï¼‰\n",
        "USE_DRIVE_DATA = False  # å¦‚æœæ•°æ®åœ¨Driveä¸­ï¼Œè®¾ç½®ä¸ºTrue\n",
        "DRIVE_DATA_PATH = \"/content/drive/MyDrive/cityscapes_data\"  # ä¿®æ”¹ä¸ºä½ çš„Driveæ•°æ®è·¯å¾„\n",
        "\n",
        "# æ–¹æ³•2ï¼šä½¿ç”¨Colabæœ¬åœ°æ•°æ®ï¼ˆå¦‚æœå·²ä¸Šä¼ æˆ–å…‹éš†ï¼‰\n",
        "LOCAL_DATA_PATH = ROOT / \"data\"\n",
        "\n",
        "# é€‰æ‹©æ•°æ®è·¯å¾„\n",
        "if USE_DRIVE_DATA and DRIVE_ROOT and Path(DRIVE_DATA_PATH).exists():\n",
        "    DATA_ROOT = Path(DRIVE_DATA_PATH)\n",
        "    print(f\"âœ… ä½¿ç”¨Google Driveæ•°æ®: {DATA_ROOT}\")\n",
        "elif LOCAL_DATA_PATH.exists():\n",
        "    DATA_ROOT = LOCAL_DATA_PATH\n",
        "    print(f\"âœ… ä½¿ç”¨æœ¬åœ°æ•°æ®: {DATA_ROOT}\")\n",
        "else:\n",
        "    # åˆ›å»ºæ•°æ®ç›®å½•ç»“æ„ï¼ˆç”¨äºåç»­ä¸Šä¼ ï¼‰\n",
        "    DATA_ROOT = ROOT / \"data\"\n",
        "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "    (DATA_ROOT / \"processed\").mkdir(exist_ok=True)\n",
        "    (DATA_ROOT / \"splits\").mkdir(exist_ok=True)\n",
        "    print(f\"âš ï¸  æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œå·²åˆ›å»º: {DATA_ROOT}\")\n",
        "    print(\"   è¯·å°†æ•°æ®ä¸Šä¼ åˆ°æ­¤ç›®å½•ï¼Œæˆ–ä¿®æ”¹ä¸Šé¢çš„è·¯å¾„è®¾ç½®\")\n",
        "\n",
        "# æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨\n",
        "SPLIT_INDEX = DATA_ROOT / \"splits\" / \"cityscapes_split_seed42.json\"\n",
        "TRAIN_LABEL_DIR = DATA_ROOT / \"processed\" / \"train\" / \"label\"\n",
        "TRAIN_PHOTO_DIR = DATA_ROOT / \"processed\" / \"train\" / \"photo\"\n",
        "VAL_LABEL_DIR = DATA_ROOT / \"processed\" / \"val\" / \"label\"\n",
        "VAL_PHOTO_DIR = DATA_ROOT / \"processed\" / \"val\" / \"photo\"\n",
        "\n",
        "print(\"\\nğŸ“‚ æ£€æŸ¥æ•°æ®æ–‡ä»¶...\")\n",
        "if SPLIT_INDEX.exists():\n",
        "    print(f\"âœ… æ•°æ®åˆ’åˆ†æ–‡ä»¶: {SPLIT_INDEX}\")\n",
        "else:\n",
        "    print(f\"âŒ æ•°æ®åˆ’åˆ†æ–‡ä»¶ç¼ºå¤±: {SPLIT_INDEX}\")\n",
        "\n",
        "for dir_path, name in [\n",
        "    (TRAIN_LABEL_DIR, \"è®­ç»ƒé›†Label\"),\n",
        "    (TRAIN_PHOTO_DIR, \"è®­ç»ƒé›†Photo\"),\n",
        "    (VAL_LABEL_DIR, \"éªŒè¯é›†Label\"),\n",
        "    (VAL_PHOTO_DIR, \"éªŒè¯é›†Photo\"),\n",
        "]:\n",
        "    if dir_path.exists():\n",
        "        file_count = len(list(dir_path.glob(\"*\")))\n",
        "        print(f\"âœ… {name}: {dir_path} ({file_count} ä¸ªæ–‡ä»¶)\")\n",
        "    else:\n",
        "        print(f\"âŒ {name} ç¼ºå¤±: {dir_path}\")\n",
        "\n",
        "# å¦‚æœæ•°æ®ä¸å­˜åœ¨ï¼Œæä¾›ä¸Šä¼ æç¤º\n",
        "if not all([SPLIT_INDEX.exists(), TRAIN_LABEL_DIR.exists(), TRAIN_PHOTO_DIR.exists()]):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ“¤ æ•°æ®ä¸Šä¼ æç¤º\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"å¦‚æœæ•°æ®åœ¨æœ¬åœ°ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸Šä¼ åˆ°Colabï¼š\")\n",
        "    print(\"1. ä½¿ç”¨Colabçš„æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼ˆé€‚åˆå°æ–‡ä»¶ï¼‰\")\n",
        "    print(\"2. ä½¿ç”¨Google Driveï¼ˆæ¨èï¼Œé€‚åˆå¤§æ–‡ä»¶ï¼‰\")\n",
        "    print(\"3. ä½¿ç”¨wgetä»URLä¸‹è½½ï¼ˆå¦‚æœæœ‰å…¬å¼€é“¾æ¥ï¼‰\")\n",
        "    print(f\"\\næ•°æ®åº”æ”¾åœ¨: {DATA_ROOT}\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„\n",
        "sys.path.insert(0, str(ROOT))\n",
        "print(f\"é¡¹ç›®æ ¹ç›®å½•ï¼š{ROOT}\")\n",
        "print(f\"å½“å‰å·¥ä½œç›®å½•ï¼š{Path.cwd()}\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "from src.models.cyclegan_generator import CycleGANGenerator\n",
        "from src.models.cyclegan_discriminator import CycleGANDiscriminator\n",
        "from src.data.dataset import CityscapesDataset\n",
        "from src.data.transforms import build_transform\n",
        "from src.eval.metrics import evaluate_batch, calculate_psnr, calculate_ssim, calculate_mae\n",
        "\n",
        "# è®¾ç½®è®¾å¤‡\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# è¾“å‡ºæ ¹ç›®å½•ï¼ˆåœ¨Colabä¸­ä¿å­˜åˆ°/content/outputsï¼Œä¹Ÿå¯ä»¥ä¿å­˜åˆ°Driveï¼‰\n",
        "# é‡è¦ï¼šcheckpointä¿å­˜åˆ°Google Driveä»¥ç¡®ä¿æŒä¹…åŒ–\n",
        "USE_DRIVE_FOR_CHECKPOINTS = True  # è®¾ç½®ä¸ºTrueä»¥å°†checkpointä¿å­˜åˆ°Drive\n",
        "\n",
        "if IN_COLAB and DRIVE_ROOT:\n",
        "    if USE_DRIVE_FOR_CHECKPOINTS:\n",
        "        # checkpointä¿å­˜åˆ°Driveï¼ˆæŒä¹…åŒ–ï¼Œä¸ä¼šå› ä¼šè¯æ–­å¼€è€Œä¸¢å¤±ï¼‰\n",
        "        CHECKPOINT_DIR = DRIVE_ROOT / \"cyclegan_checkpoints\"\n",
        "        print(f\"ğŸ“ Checkpointå°†ä¿å­˜åˆ°Google Drive: {CHECKPOINT_DIR}\")\n",
        "    else:\n",
        "        # checkpointä¿å­˜åˆ°æœ¬åœ°ï¼ˆä¸´æ—¶ï¼‰\n",
        "        CHECKPOINT_DIR = ROOT / \"outputs\" / \"checkpoints\" / \"cyclegan\"\n",
        "        print(f\"ğŸ“ Checkpointå°†ä¿å­˜åˆ°æœ¬åœ°: {CHECKPOINT_DIR}\")\n",
        "    \n",
        "    # å…¶ä»–è¾“å‡ºï¼ˆå›¾åƒã€å›¾è¡¨ã€æ—¥å¿—ï¼‰å¯ä»¥ä¿å­˜åˆ°Driveæˆ–æœ¬åœ°\n",
        "    OUTPUT_ROOT = DRIVE_ROOT / \"cyclegan_outputs\" if DRIVE_ROOT else ROOT / \"outputs\"\n",
        "    print(f\"ğŸ“ å…¶ä»–è¾“å‡ºå°†ä¿å­˜åˆ°: {OUTPUT_ROOT}\")\n",
        "else:\n",
        "    OUTPUT_ROOT = ROOT / \"outputs\"\n",
        "    CHECKPOINT_DIR = OUTPUT_ROOT / \"checkpoints\" / \"cyclegan\"\n",
        "    print(f\"ğŸ“ è¾“å‡ºå°†ä¿å­˜åˆ°æœ¬åœ°: {OUTPUT_ROOT}\")\n",
        "\n",
        "IMAGES_DIR = OUTPUT_ROOT / \"images\" / \"cyclegan\"\n",
        "FIGURES_DIR = OUTPUT_ROOT / \"figures\" / \"cyclegan\"\n",
        "LOGS_DIR = OUTPUT_ROOT / \"logs\" / \"cyclegan\"\n",
        "\n",
        "# åˆ›å»ºç›®å½•\n",
        "for d in [CHECKPOINT_DIR, IMAGES_DIR, FIGURES_DIR, LOGS_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… è·¯å¾„è®¾ç½®å®Œæˆï¼\")\n",
        "print(f\"   Checkpointç›®å½•: {CHECKPOINT_DIR}\")\n",
        "print(f\"   å›¾åƒç›®å½•: {IMAGES_DIR}\")\n",
        "print(f\"   å›¾è¡¨ç›®å½•: {FIGURES_DIR}\")\n",
        "print(f\"   æ—¥å¿—ç›®å½•: {LOGS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. æµ‹è¯•CycleGANæ¨¡å‹æ¶æ„\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºæ¨¡å‹å¹¶æµ‹è¯•å‰å‘ä¼ æ’­\n",
        "generator_G = CycleGANGenerator(in_channels=3, out_channels=3, n_residual_blocks=9).to(device)\n",
        "generator_F = CycleGANGenerator(in_channels=3, out_channels=3, n_residual_blocks=9).to(device)\n",
        "discriminator_photo = CycleGANDiscriminator(in_channels=3).to(device)\n",
        "discriminator_label = CycleGANDiscriminator(in_channels=3).to(device)\n",
        "\n",
        "generator_G.eval()\n",
        "generator_F.eval()\n",
        "discriminator_photo.eval()\n",
        "discriminator_label.eval()\n",
        "\n",
        "# æµ‹è¯•è¾“å…¥\n",
        "label = torch.randn(1, 3, 256, 256).to(device)\n",
        "photo = torch.randn(1, 3, 256, 256).to(device)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CycleGANæ¨¡å‹æ¶æ„æµ‹è¯•\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # ç”Ÿæˆå™¨Gï¼ˆLabelâ†’Photoï¼‰\n",
        "    fake_photo = generator_G(label)\n",
        "    print(f\"\\nç”Ÿæˆå™¨G (Labelâ†’Photo):\")\n",
        "    print(f\"  è¾“å…¥: {label.shape} â†’ è¾“å‡º: {fake_photo.shape}\")\n",
        "    print(f\"  è¾“å‡ºèŒƒå›´: [{fake_photo.min():.3f}, {fake_photo.max():.3f}] (æœŸæœ›: [-1, 1])\")\n",
        "    \n",
        "    # ç”Ÿæˆå™¨Fï¼ˆPhotoâ†’Labelï¼‰\n",
        "    fake_label = generator_F(photo)\n",
        "    print(f\"\\nç”Ÿæˆå™¨F (Photoâ†’Label):\")\n",
        "    print(f\"  è¾“å…¥: {photo.shape} â†’ è¾“å‡º: {fake_label.shape}\")\n",
        "    print(f\"  è¾“å‡ºèŒƒå›´: [{fake_label.min():.3f}, {fake_label.max():.3f}] (æœŸæœ›: [-1, 1])\")\n",
        "    \n",
        "    # å¾ªç¯ä¸€è‡´æ€§æµ‹è¯•\n",
        "    recovered_label = generator_F(fake_photo)\n",
        "    recovered_photo = generator_G(fake_label)\n",
        "    print(f\"\\nå¾ªç¯ä¸€è‡´æ€§æµ‹è¯•:\")\n",
        "    print(f\"  Labelâ†’Photoâ†’Label: {label.shape} â†’ {recovered_label.shape}\")\n",
        "    print(f\"  Photoâ†’Labelâ†’Photo: {photo.shape} â†’ {recovered_photo.shape}\")\n",
        "    \n",
        "    # åˆ¤åˆ«å™¨æµ‹è¯•\n",
        "    pred_real_photo = discriminator_photo(photo)\n",
        "    pred_fake_photo = discriminator_photo(fake_photo)\n",
        "    pred_real_label = discriminator_label(label)\n",
        "    pred_fake_label = discriminator_label(fake_label)\n",
        "    \n",
        "    print(f\"\\nåˆ¤åˆ«å™¨D_photo:\")\n",
        "    print(f\"  çœŸå®photo: {pred_real_photo.shape}\")\n",
        "    print(f\"  ç”Ÿæˆphoto: {pred_fake_photo.shape}\")\n",
        "    \n",
        "    print(f\"\\nåˆ¤åˆ«å™¨D_label:\")\n",
        "    print(f\"  çœŸå®label: {pred_real_label.shape}\")\n",
        "    print(f\"  ç”Ÿæˆlabel: {pred_fake_label.shape}\")\n",
        "\n",
        "# è®¡ç®—å‚æ•°é‡\n",
        "total_params_G = sum(p.numel() for p in generator_G.parameters())\n",
        "total_params_F = sum(p.numel() for p in generator_F.parameters())\n",
        "total_params_D_photo = sum(p.numel() for p in discriminator_photo.parameters())\n",
        "total_params_D_label = sum(p.numel() for p in discriminator_label.parameters())\n",
        "\n",
        "print(f\"\\nå‚æ•°é‡ç»Ÿè®¡:\")\n",
        "print(f\"  ç”Ÿæˆå™¨G: {total_params_G:,}\")\n",
        "print(f\"  ç”Ÿæˆå™¨F: {total_params_F:,}\")\n",
        "print(f\"  åˆ¤åˆ«å™¨D_photo: {total_params_D_photo:,}\")\n",
        "print(f\"  åˆ¤åˆ«å™¨D_label: {total_params_D_label:,}\")\n",
        "print(f\"  æ€»å‚æ•°é‡: {total_params_G + total_params_F + total_params_D_photo + total_params_D_label:,}\")\n",
        "\n",
        "print(\"\\nâœ… CycleGANæ¨¡å‹æ¶æ„æµ‹è¯•é€šè¿‡ï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. å‡†å¤‡æ•°æ®é›†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CycleGANæ•°æ®å¢å¼ºé…ç½®ï¼ˆè®­ç»ƒé›†ä½¿ç”¨å¼ºå¢å¼ºï¼ŒéªŒè¯é›†ä¸ä½¿ç”¨å¢å¼ºï¼‰\n",
        "train_transform = build_transform(\n",
        "    image_size=256,\n",
        "    jitter=True,\n",
        "    horizontal_flip=True,\n",
        "    color_jitter=(0.2, 0.2, 0.2, 0.05),\n",
        "    scale_range=(0.8, 1.2),\n",
        "    normalize_mode=\"tanh\"\n",
        ")\n",
        "\n",
        "val_transform = build_transform(\n",
        "    image_size=256,\n",
        "    jitter=False,\n",
        "    horizontal_flip=False,\n",
        "    color_jitter=None,\n",
        "    scale_range=None,\n",
        "    normalize_mode=\"tanh\"\n",
        ")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†\n",
        "try:\n",
        "    train_dataset = CityscapesDataset(\n",
        "        root=DATA_ROOT,\n",
        "        split=\"train\",\n",
        "        split_index=SPLIT_INDEX,\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_dataset = CityscapesDataset(\n",
        "        root=DATA_ROOT,\n",
        "        split=\"val\",\n",
        "        split_index=SPLIT_INDEX,\n",
        "        transform=val_transform\n",
        "    )\n",
        "    \n",
        "    # åœ¨Colabä¸­ï¼Œnum_workersè®¾ç½®ä¸º0ä»¥é¿å…å¤šè¿›ç¨‹é—®é¢˜\n",
        "    num_workers = 0\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=1,  # CycleGANè®ºæ–‡è®¾ç½®batch_size=1\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if device.type == \"cuda\" else False\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if device.type == \"cuda\" else False\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼\")\n",
        "    print(f\"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬\")\n",
        "    print(f\"  éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬\")\n",
        "    print(f\"  Batch size: 1 (CycleGANè®ºæ–‡è®¾ç½®)\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}\")\n",
        "    print(\"   è¯·ç¡®ä¿æ•°æ®å·²æ­£ç¡®ä¸Šä¼ åˆ°Colab\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. å®šä¹‰æŸå¤±å‡½æ•°å’Œè®­ç»ƒå‡½æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GANæŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨MSE Lossæ›¿ä»£BCE Lossï¼Œæ›´ç¨³å®šï¼‰\n",
        "class GANLoss(nn.Module):\n",
        "    \"\"\"GANæŸå¤±å‡½æ•°ï¼ˆä½¿ç”¨MSE Lossæ›¿ä»£BCE Lossï¼Œæ›´ç¨³å®šï¼‰\"\"\"\n",
        "    def __init__(self, target_real_label=1.0, target_fake_label=0.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
        "        self.loss = nn.MSELoss()\n",
        "    \n",
        "    def __call__(self, prediction, target_is_real):\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        \n",
        "        target_tensor = target_tensor.to(prediction.device).expand_as(prediction)\n",
        "        loss = self.loss(prediction, target_tensor)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def save_triplet(label, generated, ground_truth, save_path, title=\"Label | Generated | Ground Truth\"):\n",
        "    \"\"\"ä¿å­˜ä¸‰è”å›¾ï¼šLabel / Generated / Ground Truth\"\"\"\n",
        "    def tensor_to_image(tensor):\n",
        "        if tensor.dim() == 3:\n",
        "            img = tensor.permute(1, 2, 0).cpu().numpy()\n",
        "        else:\n",
        "            img = tensor[0].permute(1, 2, 0).cpu().numpy()\n",
        "        if img.min() < 0:\n",
        "            img = (img + 1) / 2\n",
        "        img = np.clip(img, 0, 1)\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "        return img\n",
        "    \n",
        "    label_img = tensor_to_image(label)\n",
        "    gen_img = tensor_to_image(generated)\n",
        "    gt_img = tensor_to_image(ground_truth)\n",
        "    \n",
        "    triplet = np.hstack([label_img, gen_img, gt_img])\n",
        "    Image.fromarray(triplet).save(save_path)\n",
        "\n",
        "\n",
        "def save_cycle_triplet(label, fake_photo, recovered_label, photo, fake_label, recovered_photo, save_path):\n",
        "    \"\"\"ä¿å­˜CycleGANçš„åŒå‘å¾ªç¯ç»“æœ\"\"\"\n",
        "    def tensor_to_image(tensor):\n",
        "        if tensor.dim() == 3:\n",
        "            img = tensor.permute(1, 2, 0).cpu().numpy()\n",
        "        else:\n",
        "            img = tensor[0].permute(1, 2, 0).cpu().numpy()\n",
        "        if img.min() < 0:\n",
        "            img = (img + 1) / 2\n",
        "        img = np.clip(img, 0, 1)\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "        return img\n",
        "    \n",
        "    label_img = tensor_to_image(label)\n",
        "    fake_photo_img = tensor_to_image(fake_photo)\n",
        "    recovered_label_img = tensor_to_image(recovered_label)\n",
        "    photo_img = tensor_to_image(photo)\n",
        "    fake_label_img = tensor_to_image(fake_label)\n",
        "    recovered_photo_img = tensor_to_image(recovered_photo)\n",
        "    \n",
        "    row1 = np.hstack([label_img, fake_photo_img, recovered_label_img])\n",
        "    row2 = np.hstack([photo_img, fake_label_img, recovered_photo_img])\n",
        "    cycle_result = np.vstack([row1, row2])\n",
        "    Image.fromarray(cycle_result).save(save_path)\n",
        "\n",
        "\n",
        "print(\"âœ… æŸå¤±å‡½æ•°å’Œå·¥å…·å‡½æ•°å®šä¹‰å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "    generator_G, generator_F,\n",
        "    discriminator_photo, discriminator_label,\n",
        "    dataloader, gan_loss, l1_loss,\n",
        "    optimizer_G, optimizer_F,\n",
        "    optimizer_D_photo, optimizer_D_label,\n",
        "    device, epoch, lambda_cycle=10.0, lambda_identity=0.5\n",
        "):\n",
        "    \"\"\"è®­ç»ƒä¸€ä¸ªepoch\"\"\"\n",
        "    generator_G.train()\n",
        "    generator_F.train()\n",
        "    discriminator_photo.train()\n",
        "    discriminator_label.train()\n",
        "    \n",
        "    total_loss_G = 0.0\n",
        "    total_loss_F = 0.0\n",
        "    total_loss_D_photo = 0.0\n",
        "    total_loss_D_label = 0.0\n",
        "    total_loss_cycle = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [Train]\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        label = batch[\"label\"].to(device)\n",
        "        photo = batch[\"photo\"].to(device)\n",
        "        \n",
        "        batch_size = label.size(0)\n",
        "        \n",
        "        # ========== è®­ç»ƒåˆ¤åˆ«å™¨ D_photo ==========\n",
        "        optimizer_D_photo.zero_grad()\n",
        "        \n",
        "        real_photo = photo\n",
        "        pred_real_photo = discriminator_photo(real_photo)\n",
        "        loss_D_photo_real = gan_loss(pred_real_photo, True)\n",
        "        \n",
        "        fake_photo = generator_G(label.detach())\n",
        "        pred_fake_photo = discriminator_photo(fake_photo.detach())\n",
        "        loss_D_photo_fake = gan_loss(pred_fake_photo, False)\n",
        "        \n",
        "        loss_D_photo = (loss_D_photo_real + loss_D_photo_fake) * 0.5\n",
        "        loss_D_photo.backward()\n",
        "        optimizer_D_photo.step()\n",
        "        \n",
        "        # ========== è®­ç»ƒåˆ¤åˆ«å™¨ D_label ==========\n",
        "        optimizer_D_label.zero_grad()\n",
        "        \n",
        "        real_label = label\n",
        "        pred_real_label = discriminator_label(real_label)\n",
        "        loss_D_label_real = gan_loss(pred_real_label, True)\n",
        "        \n",
        "        fake_label = generator_F(photo.detach())\n",
        "        pred_fake_label = discriminator_label(fake_label.detach())\n",
        "        loss_D_label_fake = gan_loss(pred_fake_label, False)\n",
        "        \n",
        "        loss_D_label = (loss_D_label_real + loss_D_label_fake) * 0.5\n",
        "        loss_D_label.backward()\n",
        "        optimizer_D_label.step()\n",
        "        \n",
        "        # ========== è®­ç»ƒç”Ÿæˆå™¨ G å’Œ F ==========\n",
        "        optimizer_G.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        \n",
        "        # å¯¹æŠ—æŸå¤±\n",
        "        fake_photo = generator_G(label)\n",
        "        pred_fake_photo = discriminator_photo(fake_photo)\n",
        "        loss_GAN_G = gan_loss(pred_fake_photo, True)\n",
        "        \n",
        "        fake_label = generator_F(photo)\n",
        "        pred_fake_label = discriminator_label(fake_label)\n",
        "        loss_GAN_F = gan_loss(pred_fake_label, True)\n",
        "        \n",
        "        # å¾ªç¯ä¸€è‡´æ€§æŸå¤±\n",
        "        recovered_label = generator_F(fake_photo)\n",
        "        loss_cycle_forward = l1_loss(recovered_label, label) * lambda_cycle\n",
        "        \n",
        "        recovered_photo = generator_G(fake_label)\n",
        "        loss_cycle_backward = l1_loss(recovered_photo, photo) * lambda_cycle\n",
        "        \n",
        "        # èº«ä»½æŸå¤±ï¼ˆå¯é€‰ï¼‰\n",
        "        if lambda_identity > 0:\n",
        "            identity_photo = generator_G(photo)\n",
        "            loss_identity_G = l1_loss(identity_photo, photo) * lambda_identity\n",
        "            \n",
        "            identity_label = generator_F(label)\n",
        "            loss_identity_F = l1_loss(identity_label, label) * lambda_identity\n",
        "        else:\n",
        "            loss_identity_G = torch.tensor(0.0, device=device)\n",
        "            loss_identity_F = torch.tensor(0.0, device=device)\n",
        "        \n",
        "        # é‡æ–°è®¡ç®—ä»¥åˆ†ç¦»è®¡ç®—å›¾\n",
        "        fake_label_detached = fake_label.detach()\n",
        "        recovered_photo_G = generator_G(fake_label_detached)\n",
        "        loss_cycle_backward_G = l1_loss(recovered_photo_G, photo) * lambda_cycle\n",
        "        \n",
        "        fake_photo_detached = fake_photo.detach()\n",
        "        recovered_label_F = generator_F(fake_photo_detached)\n",
        "        loss_cycle_forward_F = l1_loss(recovered_label_F, label) * lambda_cycle\n",
        "        \n",
        "        loss_G = loss_GAN_G + loss_cycle_backward_G + loss_identity_G\n",
        "        loss_F = loss_GAN_F + loss_cycle_forward_F + loss_identity_F\n",
        "        \n",
        "        loss_G.backward()\n",
        "        loss_F.backward()\n",
        "        \n",
        "        optimizer_G.step()\n",
        "        optimizer_F.step()\n",
        "        \n",
        "        # è®°å½•æŸå¤±\n",
        "        total_loss_G += loss_G.item()\n",
        "        total_loss_F += loss_F.item()\n",
        "        total_loss_D_photo += loss_D_photo.item()\n",
        "        total_loss_D_label += loss_D_label.item()\n",
        "        total_loss_cycle += (loss_cycle_forward.item() + loss_cycle_backward.item())\n",
        "        num_batches += 1\n",
        "        \n",
        "        cycle_loss_total = loss_cycle_forward.item() + loss_cycle_backward.item()\n",
        "        pbar.set_postfix({\n",
        "            \"G\": f\"{loss_G.item():.4f}\",\n",
        "            \"F\": f\"{loss_F.item():.4f}\",\n",
        "            \"D_photo\": f\"{loss_D_photo.item():.4f}\",\n",
        "            \"D_label\": f\"{loss_D_label.item():.4f}\",\n",
        "            \"Cycle\": f\"{cycle_loss_total:.4f}\",\n",
        "        })\n",
        "    \n",
        "    avg_loss_G = total_loss_G / num_batches if num_batches > 0 else 0.0\n",
        "    avg_loss_F = total_loss_F / num_batches if num_batches > 0 else 0.0\n",
        "    avg_loss_D_photo = total_loss_D_photo / num_batches if num_batches > 0 else 0.0\n",
        "    avg_loss_D_label = total_loss_D_label / num_batches if num_batches > 0 else 0.0\n",
        "    avg_loss_cycle = total_loss_cycle / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        \"loss_G\": avg_loss_G,\n",
        "        \"loss_F\": avg_loss_F,\n",
        "        \"loss_D_photo\": avg_loss_D_photo,\n",
        "        \"loss_D_label\": avg_loss_D_label,\n",
        "        \"loss_cycle\": avg_loss_cycle,\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(\n",
        "    generator_G, generator_F,\n",
        "    dataloader, l1_loss, device, epoch, output_dir,\n",
        "    num_samples=10\n",
        "):\n",
        "    \"\"\"éªŒè¯å¹¶ä¿å­˜ä¸‰è”å›¾\"\"\"\n",
        "    generator_G.eval()\n",
        "    generator_F.eval()\n",
        "    \n",
        "    total_loss_cycle = 0.0\n",
        "    total_metrics = {\"psnr\": 0.0, \"ssim\": 0.0, \"mae\": 0.0}\n",
        "    num_batches = 0\n",
        "    saved_samples = 0\n",
        "    \n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch} [Val]\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        label = batch[\"label\"].to(device)\n",
        "        photo = batch[\"photo\"].to(device)\n",
        "        name = batch[\"name\"]\n",
        "        \n",
        "        fake_photo = generator_G(label)\n",
        "        fake_label = generator_F(photo)\n",
        "        \n",
        "        recovered_label = generator_F(fake_photo)\n",
        "        recovered_photo = generator_G(fake_label)\n",
        "        \n",
        "        loss_cycle_forward = l1_loss(recovered_label, label)\n",
        "        loss_cycle_backward = l1_loss(recovered_photo, photo)\n",
        "        loss_cycle = loss_cycle_forward + loss_cycle_backward\n",
        "        \n",
        "        total_loss_cycle += loss_cycle.item()\n",
        "        \n",
        "        metrics = evaluate_batch(fake_photo, photo)\n",
        "        for key in total_metrics:\n",
        "            total_metrics[key] += metrics[key]\n",
        "        \n",
        "        num_batches += 1\n",
        "        \n",
        "        if saved_samples < num_samples:\n",
        "            save_path = output_dir / \"images\" / f\"epoch_{epoch:03d}_sample_{saved_samples:02d}_label2photo.png\"\n",
        "            save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            save_triplet(label, fake_photo, photo, save_path, \"Label | Generated Photo | Ground Truth Photo\")\n",
        "            \n",
        "            save_path = output_dir / \"images\" / f\"epoch_{epoch:03d}_sample_{saved_samples:02d}_photo2label.png\"\n",
        "            save_triplet(photo, fake_label, label, save_path, \"Photo | Generated Label | Ground Truth Label\")\n",
        "            \n",
        "            save_path = output_dir / \"images\" / f\"epoch_{epoch:03d}_sample_{saved_samples:02d}_cycle.png\"\n",
        "            save_cycle_triplet(\n",
        "                label, fake_photo, recovered_label,\n",
        "                photo, fake_label, recovered_photo,\n",
        "                save_path\n",
        "            )\n",
        "            \n",
        "            saved_samples += 1\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            \"cycle_loss\": f\"{loss_cycle.item():.4f}\",\n",
        "            \"psnr\": f\"{metrics['psnr']:.2f}\",\n",
        "        })\n",
        "    \n",
        "    avg_loss_cycle = total_loss_cycle / num_batches if num_batches > 0 else 0.0\n",
        "    avg_metrics = {key: val / num_batches for key, val in total_metrics.items()}\n",
        "    \n",
        "    return avg_loss_cycle, avg_metrics\n",
        "\n",
        "\n",
        "print(\"âœ… è®­ç»ƒå’ŒéªŒè¯å‡½æ•°å®šä¹‰å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. å¼€å§‹è®­ç»ƒ\n",
        "\n",
        "**æ³¨æ„**ï¼šè®­ç»ƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®ï¼š\n",
        "- ä½¿ç”¨GPUè¿è¡Œæ—¶\n",
        "- å®šæœŸä¿å­˜checkpointï¼ˆæ¯10ä¸ªepochï¼‰\n",
        "- å¦‚æœColabä¼šè¯æ–­å¼€ï¼Œå¯ä»¥ä»checkpointæ¢å¤è®­ç»ƒ\n",
        "\n",
        "### Checkpointç®¡ç†è¯´æ˜\n",
        "\n",
        "**ä¿å­˜ä½ç½®**ï¼š\n",
        "- Checkpointè‡ªåŠ¨ä¿å­˜åˆ°Google Driveçš„ `cyclegan_checkpoints` æ–‡ä»¶å¤¹\n",
        "- è·¯å¾„ï¼š`/content/drive/MyDrive/cyclegan_checkpoints/`\n",
        "\n",
        "**æ¢å¤è®­ç»ƒ**ï¼š\n",
        "1. åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæ‰¾åˆ° `RESUME_EPOCH` å‚æ•°\n",
        "2. è®¾ç½®ä¸ºè¦æ¢å¤çš„epochç¼–å·ï¼ˆä¾‹å¦‚ï¼š`RESUME_EPOCH = 20`ï¼‰\n",
        "3. ç¡®ä¿å¯¹åº”çš„checkpointæ–‡ä»¶å·²å­˜åœ¨äºGoogle Driveä¸­\n",
        "\n",
        "**Checkpointæ–‡ä»¶è¯´æ˜**ï¼š\n",
        "- æ¯ä¸ªcheckpointæ–‡ä»¶åŒ…å«å®Œæ•´çš„æ¨¡å‹çŠ¶æ€ï¼ˆç”Ÿæˆå™¨ã€åˆ¤åˆ«å™¨ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨ç­‰ï¼‰\n",
        "- é€šå¸¸åªéœ€è¦ä¿ç•™æœ€æ–°çš„checkpointï¼ˆå¦‚epoch 20ï¼‰ï¼Œå› ä¸ºå¯ä»¥ä»ä»»æ„checkpointæ¢å¤\n",
        "- æ–‡ä»¶åæ ¼å¼ï¼š`checkpoint_epoch_020.pth`ï¼ˆ3ä½æ•°å­—ï¼Œè¡¥é›¶ï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# è®­ç»ƒé…ç½®\n",
        "# ============================================\n",
        "epochs = 200\n",
        "batch_size = 1  # CycleGANè®ºæ–‡è®¾ç½®\n",
        "lr = 2e-4\n",
        "start_decay_epoch = 100\n",
        "lambda_cycle = 10.0\n",
        "lambda_identity = 0.5\n",
        "n_residual_blocks = 9\n",
        "\n",
        "# åˆ›å»ºæ¨¡å‹\n",
        "print(\"åˆ›å»ºæ¨¡å‹...\")\n",
        "generator_G = CycleGANGenerator(\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    n_residual_blocks=n_residual_blocks\n",
        ").to(device)\n",
        "generator_F = CycleGANGenerator(\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    n_residual_blocks=n_residual_blocks\n",
        ").to(device)\n",
        "discriminator_photo = CycleGANDiscriminator(in_channels=3).to(device)\n",
        "discriminator_label = CycleGANDiscriminator(in_channels=3).to(device)\n",
        "\n",
        "# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
        "gan_loss = GANLoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "\n",
        "optimizer_G = optim.Adam(generator_G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_F = optim.Adam(generator_F.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_photo = optim.Adam(discriminator_photo.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_label = optim.Adam(discriminator_label.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# å­¦ä¹ ç‡è°ƒåº¦\n",
        "def get_lr_scheduler(optimizer, num_epochs, start_decay_epoch):\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < start_decay_epoch:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 1.0 - (epoch - start_decay_epoch) / (num_epochs - start_decay_epoch)\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler_G = get_lr_scheduler(optimizer_G, epochs, start_decay_epoch)\n",
        "scheduler_F = get_lr_scheduler(optimizer_F, epochs, start_decay_epoch)\n",
        "scheduler_D_photo = get_lr_scheduler(optimizer_D_photo, epochs, start_decay_epoch)\n",
        "scheduler_D_label = get_lr_scheduler(optimizer_D_label, epochs, start_decay_epoch)\n",
        "\n",
        "# ============================================\n",
        "# æ¢å¤è®­ç»ƒï¼ˆä»checkpointç»§ç»­ï¼‰\n",
        "# ============================================\n",
        "# æŒ‡å®šè¦æ¢å¤çš„checkpointè·¯å¾„\n",
        "# å¦‚æœcheckpointåœ¨Google Driveä¸­ï¼Œç›´æ¥æŒ‡å®šæ–‡ä»¶åå³å¯ï¼ˆä¼šè‡ªåŠ¨åœ¨CHECKPOINT_DIRä¸­æŸ¥æ‰¾ï¼‰\n",
        "# ä¾‹å¦‚ï¼šä»epoch 20ç»§ç»­è®­ç»ƒ\n",
        "RESUME_EPOCH = 20  # è®¾ç½®ä¸ºNoneè¡¨ç¤ºä»å¤´å¼€å§‹ï¼Œæˆ–è®¾ç½®ä¸ºè¦æ¢å¤çš„epochç¼–å·\n",
        "# æˆ–è€…ç›´æ¥æŒ‡å®šcheckpointæ–‡ä»¶åï¼š\n",
        "# resume_checkpoint = CHECKPOINT_DIR / \"checkpoint_epoch_020.pth\"\n",
        "\n",
        "if RESUME_EPOCH is not None:\n",
        "    resume_checkpoint = CHECKPOINT_DIR / f\"checkpoint_epoch_{RESUME_EPOCH:03d}.pth\"\n",
        "    print(f\"ğŸ“Œ å‡†å¤‡ä»epoch {RESUME_EPOCH}æ¢å¤è®­ç»ƒ\")\n",
        "    print(f\"   Checkpointè·¯å¾„: {resume_checkpoint}\")\n",
        "    if not resume_checkpoint.exists():\n",
        "        print(f\"âš ï¸  è­¦å‘Šï¼šcheckpointæ–‡ä»¶ä¸å­˜åœ¨: {resume_checkpoint}\")\n",
        "        print(\"   è¯·ç¡®ä¿ï¼š\")\n",
        "        print(\"   1. Google Driveå·²æ­£ç¡®æŒ‚è½½\")\n",
        "        print(\"   2. checkpointæ–‡ä»¶å·²ä¸Šä¼ åˆ°Driveçš„checkpointç›®å½•\")\n",
        "        print(\"   3. æ–‡ä»¶åæ ¼å¼æ­£ç¡®ï¼ˆå¦‚ï¼šcheckpoint_epoch_020.pthï¼‰\")\n",
        "        resume_checkpoint = None\n",
        "else:\n",
        "    resume_checkpoint = None\n",
        "\n",
        "start_epoch = 1\n",
        "\n",
        "# åˆå§‹åŒ–è®­ç»ƒå†å²åˆ—è¡¨\n",
        "train_losses_G = []\n",
        "train_losses_F = []\n",
        "train_losses_D_photo = []\n",
        "train_losses_D_label = []\n",
        "train_losses_cycle = []\n",
        "val_losses_cycle = []\n",
        "val_psnrs = []\n",
        "val_ssims = []\n",
        "val_maes = []\n",
        "\n",
        "# å°è¯•ä»checkpointæ¢å¤\n",
        "if resume_checkpoint is not None and Path(resume_checkpoint).exists():\n",
        "    print(f\"ğŸ” æ¢å¤è®­ç»ƒè‡ª: {resume_checkpoint}\")\n",
        "    checkpoint = torch.load(resume_checkpoint, map_location=device, weights_only=False)\n",
        "    \n",
        "    generator_G.load_state_dict(checkpoint[\"generator_G_state_dict\"])\n",
        "    generator_F.load_state_dict(checkpoint[\"generator_F_state_dict\"])\n",
        "    discriminator_photo.load_state_dict(checkpoint[\"discriminator_photo_state_dict\"])\n",
        "    discriminator_label.load_state_dict(checkpoint[\"discriminator_label_state_dict\"])\n",
        "    optimizer_G.load_state_dict(checkpoint[\"optimizer_G_state_dict\"])\n",
        "    optimizer_F.load_state_dict(checkpoint[\"optimizer_F_state_dict\"])\n",
        "    optimizer_D_photo.load_state_dict(checkpoint[\"optimizer_D_photo_state_dict\"])\n",
        "    optimizer_D_label.load_state_dict(checkpoint[\"optimizer_D_label_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    \n",
        "    for _ in range(start_epoch - 1):\n",
        "        scheduler_G.step()\n",
        "        scheduler_F.step()\n",
        "        scheduler_D_photo.step()\n",
        "        scheduler_D_label.step()\n",
        "    \n",
        "    history_path = LOGS_DIR / \"history.json\"\n",
        "    if history_path.exists():\n",
        "        with open(history_path, \"r\") as f:\n",
        "            history = json.load(f)\n",
        "        train_losses_G = history.get(\"train_losses_G\", [])\n",
        "        train_losses_F = history.get(\"train_losses_F\", [])\n",
        "        train_losses_D_photo = history.get(\"train_losses_D_photo\", [])\n",
        "        train_losses_D_label = history.get(\"train_losses_D_label\", [])\n",
        "        train_losses_cycle = history.get(\"train_losses_cycle\", [])\n",
        "        val_losses_cycle = history.get(\"val_losses_cycle\", [])\n",
        "        val_psnrs = history.get(\"val_psnrs\", [])\n",
        "        val_ssims = history.get(\"val_ssims\", [])\n",
        "        val_maes = history.get(\"val_maes\", [])\n",
        "        print(f\"âœ… å·²æ¢å¤è®­ç»ƒå†å²ï¼ˆ{len(train_losses_G)} ä¸ªepochï¼‰\")\n",
        "    \n",
        "    print(f\"ğŸ“Œ å°†ä» Epoch {start_epoch} ç»§ç»­è®­ç»ƒåˆ° Epoch {epochs}\")\n",
        "else:\n",
        "    print(f\"âš ï¸  æœªæ‰¾åˆ°checkpointæˆ–æœªæŒ‡å®šcheckpoint\")\n",
        "    print(\"   å°†ä» Epoch 1 å¼€å§‹å…¨æ–°è®­ç»ƒ\")\n",
        "\n",
        "print(f\"\\nğŸš€ Starting CycleGAN training...\")\n",
        "print(f\"   Epochs: {epochs}, Batch size: {batch_size}, LR: {lr}\")\n",
        "print(f\"   Lambda cycle: {lambda_cycle}, Lambda identity: {lambda_identity}\")\n",
        "print(f\"   Residual blocks: {n_residual_blocks}\")\n",
        "print(f\"   Checkpointä¿å­˜è·¯å¾„: {CHECKPOINT_DIR}\")\n",
        "\n",
        "# è®­ç»ƒå¾ªç¯\n",
        "best_val_psnr = 0.0\n",
        "\n",
        "for epoch in range(start_epoch, epochs + 1):\n",
        "    # è®­ç»ƒ\n",
        "    train_losses = train_epoch(\n",
        "        generator_G, generator_F,\n",
        "        discriminator_photo, discriminator_label,\n",
        "        train_loader, gan_loss, l1_loss,\n",
        "        optimizer_G, optimizer_F,\n",
        "        optimizer_D_photo, optimizer_D_label,\n",
        "        device, epoch,\n",
        "        lambda_cycle=lambda_cycle,\n",
        "        lambda_identity=lambda_identity\n",
        "    )\n",
        "    \n",
        "    # éªŒè¯\n",
        "    val_loss_cycle, val_metrics = validate(\n",
        "        generator_G, generator_F,\n",
        "        val_loader, l1_loss, device, epoch, OUTPUT_ROOT / \"cyclegan\",\n",
        "        num_samples=10\n",
        "    )\n",
        "    \n",
        "    # å­¦ä¹ ç‡è°ƒåº¦\n",
        "    scheduler_G.step()\n",
        "    scheduler_F.step()\n",
        "    scheduler_D_photo.step()\n",
        "    scheduler_D_label.step()\n",
        "    current_lr = optimizer_G.param_groups[0][\"lr\"]\n",
        "    \n",
        "    # è®°å½•å†å²\n",
        "    train_losses_G.append(train_losses[\"loss_G\"])\n",
        "    train_losses_F.append(train_losses[\"loss_F\"])\n",
        "    train_losses_D_photo.append(train_losses[\"loss_D_photo\"])\n",
        "    train_losses_D_label.append(train_losses[\"loss_D_label\"])\n",
        "    train_losses_cycle.append(train_losses[\"loss_cycle\"])\n",
        "    val_losses_cycle.append(val_loss_cycle)\n",
        "    val_psnrs.append(val_metrics[\"psnr\"])\n",
        "    val_ssims.append(val_metrics[\"ssim\"])\n",
        "    val_maes.append(val_metrics[\"mae\"])\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
        "        print(f\"  Train - G: {train_losses['loss_G']:.4f}, F: {train_losses['loss_F']:.4f}, \"\n",
        "              f\"D_photo: {train_losses['loss_D_photo']:.4f}, D_label: {train_losses['loss_D_label']:.4f}, \"\n",
        "              f\"Cycle: {train_losses['loss_cycle']:.4f}\")\n",
        "        print(f\"  Val - Cycle Loss: {val_loss_cycle:.4f}, PSNR: {val_metrics['psnr']:.2f}dB, \"\n",
        "              f\"SSIM: {val_metrics['ssim']:.4f}, MAE: {val_metrics['mae']:.4f}, LR: {current_lr:.6f}\")\n",
        "        \n",
        "        # ä¿å­˜checkpoint\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"generator_G_state_dict\": generator_G.state_dict(),\n",
        "            \"generator_F_state_dict\": generator_F.state_dict(),\n",
        "            \"discriminator_photo_state_dict\": discriminator_photo.state_dict(),\n",
        "            \"discriminator_label_state_dict\": discriminator_label.state_dict(),\n",
        "            \"optimizer_G_state_dict\": optimizer_G.state_dict(),\n",
        "            \"optimizer_F_state_dict\": optimizer_F.state_dict(),\n",
        "            \"optimizer_D_photo_state_dict\": optimizer_D_photo.state_dict(),\n",
        "            \"optimizer_D_label_state_dict\": optimizer_D_label.state_dict(),\n",
        "            \"scheduler_G_state_dict\": scheduler_G.state_dict(),\n",
        "            \"scheduler_F_state_dict\": scheduler_F.state_dict(),\n",
        "            \"scheduler_D_photo_state_dict\": scheduler_D_photo.state_dict(),\n",
        "            \"scheduler_D_label_state_dict\": scheduler_D_label.state_dict(),\n",
        "            \"val_psnr\": val_metrics[\"psnr\"],\n",
        "            \"val_ssim\": val_metrics[\"ssim\"],\n",
        "            \"val_mae\": val_metrics[\"mae\"],\n",
        "        }, CHECKPOINT_DIR / f\"checkpoint_epoch_{epoch:03d}.pth\")\n",
        "        \n",
        "        # ä¿å­˜è®­ç»ƒå†å²\n",
        "        history = {\n",
        "            \"train_losses_G\": train_losses_G,\n",
        "            \"train_losses_F\": train_losses_F,\n",
        "            \"train_losses_D_photo\": train_losses_D_photo,\n",
        "            \"train_losses_D_label\": train_losses_D_label,\n",
        "            \"train_losses_cycle\": train_losses_cycle,\n",
        "            \"val_losses_cycle\": val_losses_cycle,\n",
        "            \"val_psnrs\": val_psnrs,\n",
        "            \"val_ssims\": val_ssims,\n",
        "            \"val_maes\": val_maes,\n",
        "        }\n",
        "        with open(LOGS_DIR / \"history.json\", \"w\") as f:\n",
        "            json.dump(history, f, indent=2)\n",
        "        \n",
        "        checkpoint_path = CHECKPOINT_DIR / f\"checkpoint_epoch_{epoch:03d}.pth\"\n",
        "        print(f\"âœ… Checkpointå·²ä¿å­˜: {checkpoint_path}\")\n",
        "        \n",
        "        # å¦‚æœä¿å­˜åˆ°Driveï¼Œæç¤ºç”¨æˆ·\n",
        "        if IN_COLAB and USE_DRIVE_FOR_CHECKPOINTS and DRIVE_ROOT:\n",
        "            print(f\"   ğŸ’¾ å·²ä¿å­˜åˆ°Google Driveï¼Œå³ä½¿ä¼šè¯æ–­å¼€ä¹Ÿä¸ä¼šä¸¢å¤±\")\n",
        "\n",
        "# ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
        "torch.save({\n",
        "    \"generator_G_state_dict\": generator_G.state_dict(),\n",
        "    \"generator_F_state_dict\": generator_F.state_dict(),\n",
        "    \"discriminator_photo_state_dict\": discriminator_photo.state_dict(),\n",
        "    \"discriminator_label_state_dict\": discriminator_label.state_dict(),\n",
        "}, CHECKPOINT_DIR / \"model_final.pth\")\n",
        "\n",
        "print(f\"\\nâœ… Training complete!\")\n",
        "print(f\"   Final Val Cycle Loss: {val_losses_cycle[-1]:.4f}\")\n",
        "print(f\"   Final PSNR: {val_psnrs[-1]:.2f}dB\")\n",
        "print(f\"   Final SSIM: {val_ssims[-1]:.4f}\")\n",
        "print(f\"   Final MAE: {val_maes[-1]:.4f}\")\n",
        "print(f\"\\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_ROOT}\")\n",
        "print(f\"ğŸ“ Checkpointä¿å­˜åœ¨: {CHECKPOINT_DIR}\")\n",
        "if IN_COLAB and USE_DRIVE_FOR_CHECKPOINTS and DRIVE_ROOT:\n",
        "    print(f\"   âœ… Checkpointå·²ä¿å­˜åˆ°Google Driveï¼Œå³ä½¿Colabä¼šè¯æ–­å¼€ä¹Ÿä¸ä¼šä¸¢å¤±\")\n",
        "    print(f\"   ğŸ’¡ ä¸‹æ¬¡è®­ç»ƒæ—¶ï¼Œè®¾ç½® RESUME_EPOCH = {epochs} å³å¯ä»æ­¤checkpointç»§ç»­è®­ç»ƒ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿\n",
        "if len(train_losses_G) > 0:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    \n",
        "    epochs_range = range(1, len(train_losses_G) + 1)\n",
        "    \n",
        "    # ç”Ÿæˆå™¨æŸå¤±\n",
        "    axes[0, 0].plot(epochs_range, train_losses_G, label=\"Generator G\", color=\"blue\")\n",
        "    axes[0, 0].plot(epochs_range, train_losses_F, label=\"Generator F\", color=\"green\")\n",
        "    axes[0, 0].set_xlabel(\"Epoch\")\n",
        "    axes[0, 0].set_ylabel(\"Loss\")\n",
        "    axes[0, 0].set_title(\"Generator Losses\")\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # åˆ¤åˆ«å™¨æŸå¤±\n",
        "    axes[0, 1].plot(epochs_range, train_losses_D_photo, label=\"Discriminator Photo\", color=\"red\")\n",
        "    axes[0, 1].plot(epochs_range, train_losses_D_label, label=\"Discriminator Label\", color=\"orange\")\n",
        "    axes[0, 1].set_xlabel(\"Epoch\")\n",
        "    axes[0, 1].set_ylabel(\"Loss\")\n",
        "    axes[0, 1].set_title(\"Discriminator Losses\")\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # å¾ªç¯ä¸€è‡´æ€§æŸå¤±\n",
        "    axes[0, 2].plot(epochs_range, train_losses_cycle, label=\"Train Cycle Loss\", color=\"purple\")\n",
        "    axes[0, 2].plot(epochs_range, val_losses_cycle, label=\"Val Cycle Loss\", color=\"magenta\", linestyle=\"--\")\n",
        "    axes[0, 2].set_xlabel(\"Epoch\")\n",
        "    axes[0, 2].set_ylabel(\"Loss\")\n",
        "    axes[0, 2].set_title(\"Cycle Consistency Loss\")\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # PSNR\n",
        "    axes[1, 0].plot(epochs_range, val_psnrs, label=\"PSNR\", color=\"blue\")\n",
        "    axes[1, 0].set_xlabel(\"Epoch\")\n",
        "    axes[1, 0].set_ylabel(\"PSNR (dB)\")\n",
        "    axes[1, 0].set_title(\"Validation PSNR\")\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # SSIM\n",
        "    axes[1, 1].plot(epochs_range, val_ssims, label=\"SSIM\", color=\"green\")\n",
        "    axes[1, 1].set_xlabel(\"Epoch\")\n",
        "    axes[1, 1].set_ylabel(\"SSIM\")\n",
        "    axes[1, 1].set_title(\"Validation SSIM\")\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # MAE\n",
        "    axes[1, 2].plot(epochs_range, val_maes, label=\"MAE\", color=\"red\")\n",
        "    axes[1, 2].set_xlabel(\"Epoch\")\n",
        "    axes[1, 2].set_ylabel(\"MAE\")\n",
        "    axes[1, 2].set_title(\"Validation MAE\")\n",
        "    axes[1, 2].legend()\n",
        "    axes[1, 2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / \"training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"è¯·å…ˆå®Œæˆè®­ç»ƒ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½ä¿å­˜çš„ä¸‰è”å›¾å¹¶å±•ç¤º\n",
        "import glob\n",
        "\n",
        "sample_images_label2photo = sorted(glob.glob(str(IMAGES_DIR / \"epoch_*_sample_*_label2photo.png\")))\n",
        "sample_images_cycle = sorted(glob.glob(str(IMAGES_DIR / \"epoch_*_sample_*_cycle.png\")))\n",
        "\n",
        "if sample_images_label2photo:\n",
        "    # æ˜¾ç¤ºLabelâ†’Photoæ–¹å‘çš„æ ·ä¾‹\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(\"CycleGAN Generated Samples (Label â†’ Photo)\", fontsize=16)\n",
        "    \n",
        "    for idx, img_path in enumerate(sample_images_label2photo[:6]):\n",
        "        row = idx // 3\n",
        "        col = idx % 3\n",
        "        img = Image.open(img_path)\n",
        "        axes[row, col].imshow(img)\n",
        "        axes[row, col].set_title(f\"Sample {idx + 1}\")\n",
        "        axes[row, col].axis(\"off\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / \"samples_label2photo.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    \n",
        "    # æ˜¾ç¤ºå¾ªç¯ä¸€è‡´æ€§ç»“æœ\n",
        "    if sample_images_cycle:\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(\"CycleGAN Cycle Consistency Results\", fontsize=16)\n",
        "        \n",
        "        for idx, img_path in enumerate(sample_images_cycle[:6]):\n",
        "            row = idx // 3\n",
        "            col = idx % 3\n",
        "            img = Image.open(img_path)\n",
        "            axes[row, col].imshow(img)\n",
        "            axes[row, col].set_title(f\"Sample {idx + 1}\")\n",
        "            axes[row, col].axis(\"off\")\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIGURES_DIR / \"samples_cycle.png\", dpi=150, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"æœªæ‰¾åˆ°æ ·ä¾‹å›¾åƒã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ä¸‹è½½ç»“æœï¼ˆå¯é€‰ï¼‰\n",
        "\n",
        "è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥å°†ç»“æœä¸‹è½½åˆ°æœ¬åœ°ï¼š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¦‚æœç»“æœä¿å­˜åœ¨Colabæœ¬åœ°ï¼ˆ/content/outputsï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ä¸‹è½½\n",
        "# æ³¨æ„ï¼šå¦‚æœç»“æœå·²ä¿å­˜åˆ°Google Driveï¼Œå¯ä»¥ç›´æ¥ä»Driveä¸‹è½½\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"ğŸ“¥ ä¸‹è½½ç»“æœæ–‡ä»¶...\")\n",
        "    print(f\"ç»“æœä¿å­˜åœ¨: {OUTPUT_ROOT}\")\n",
        "    print(\"\\nä¸‹è½½æ–¹æ³•ï¼š\")\n",
        "    print(\"1. ä½¿ç”¨Colabçš„æ–‡ä»¶æµè§ˆå™¨ï¼ˆå·¦ä¾§æ–‡ä»¶å›¾æ ‡ï¼‰\")\n",
        "    print(\"2. å³é”®ç‚¹å‡»æ–‡ä»¶/æ–‡ä»¶å¤¹ â†’ ä¸‹è½½\")\n",
        "    print(\"3. æˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºzipå‹ç¼©åŒ…ï¼š\")\n",
        "    print(f\"   !zip -r cyclegan_results.zip {OUTPUT_ROOT}\")\n",
        "    \n",
        "    # å¯é€‰ï¼šè‡ªåŠ¨åˆ›å»ºzipæ–‡ä»¶\n",
        "    # import shutil\n",
        "    # shutil.make_archive(\"cyclegan_results\", \"zip\", OUTPUT_ROOT)\n",
        "    # print(\"âœ… å·²åˆ›å»ºå‹ç¼©åŒ…: cyclegan_results.zip\")\n",
        "else:\n",
        "    print(\"æœ¬åœ°ç¯å¢ƒï¼Œç»“æœå·²ä¿å­˜åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
